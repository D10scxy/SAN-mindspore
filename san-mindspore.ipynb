{"metadata": {"kernelspec": {"display_name": "MindSpore-python3.7-aarch64", "language": "python", "name": "mindspore-python3.7-aarch64"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.6"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# SAN - Mindspore", "metadata": {}}, {"cell_type": "markdown", "source": "## \u5bfc\u5165\u4f9d\u8d56", "metadata": {}}, {"cell_type": "code", "source": "import os\nimport numpy as np\nimport re\nimport sys\nimport random\nimport unicodedata\nimport math\n\nfrom mindspore import Tensor, nn, Model, context, DatasetHelper, ms_function\nfrom mindspore.train.serialization import load_param_into_net, load_checkpoint, save_checkpoint\nfrom mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\nfrom mindspore import dataset as ds\nfrom mindspore.mindrecord import FileWriter\nfrom mindspore import Parameter\nfrom mindspore.nn.loss.loss import _Loss\nfrom mindspore.ops import functional as F\nfrom mindspore.ops import operations as P\nfrom mindspore.common import dtype as mstype\n\nimport moxing as mox\nmox.file.copy_parallel(src_url=\"s3://nlp---3190101095/nlp/vqa/data/annotations/\", dst_url='./data/annotations/')\nmox.file.copy_parallel(src_url=\"s3://nlp---3190101095/nlp/vqa/data/questions/\", dst_url='./data/questions/')\nmox.file.copy_parallel(src_url=\"s3://nlp---3190101095/nlp/vqa/VQA/\", dst_url='./VQA/')\nmox.file.copy_parallel(src_url=\"s3://nlp---3190101095/nlp/vqa/train_feat.pkl\", dst_url='./train_feat.pkl')\nmox.file.copy_parallel(src_url=\"s3://nlp---3190101095/nlp/vqa/val_feat.pkl\", dst_url='./val_feat.pkl')\n\nfrom VQA.preprocess_data import preProcess\nfrom VQA.vqa import *\nfrom VQA.vqaEval import *\n\ncontext.set_context(mode=context.GRAPH_MODE, device_target='Ascend')", "metadata": {"trusted": true}, "execution_count": 1, "outputs": [{"name": "stderr", "text": "INFO:root:Using MoXing-v2.0.0.rc2.4b57a67b-4b57a67b\nINFO:root:Using OBS-Python-SDK-3.20.9.1\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "from easydict import EasyDict as edict\n\n# CONFIG\ncfg = edict({\n    'trainFeatFile': './train_feat.pkl',\n    'valFeatFile': './val_feat.pkl',\n    'trainAnnFile': './data/annotations/train.json',\n    'trainQuesFile': './data/questions/train.json',\n    'valAnnFile': './data/annotations/val.json',\n    'valQuesFile': './data/questions/val.json',\n    'resultFile': 'result.json',\n    'trainSize': 4096,\n    'valSize': 1024,\n    'max_seq_length': 12,\n    'hidden_size': 1024,\n    'batch_size': 32,\n    'eval_batch_size': 1,\n    'learning_rate': 0.001,\n    'momentum': 0.9,\n    'num_epochs': 15,\n    'vocab_size': 0,\n    'checkpointFile': './san.ckpt'\n})", "metadata": {"trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "code", "source": "EOS = \"<eos>\"\nSOS = \"<sos>\"\nMAX_SEQ_LEN=12\n\ndef prepareVocab(data):\n    vocab = set(' '.join(data).split(' '))\n    id2word = [EOS] + [SOS] + list(vocab)\n    word2id = {c:i for i,c in enumerate(id2word)}\n    vocab_size = len(id2word)\n    print(\"Finish prepare Vocab. Size: %d\" %(vocab_size))\n    cfg.vocab_size = vocab_size\n    return id2word, word2id\n\ndef wordEncode(text, word2id):\n    data = [1] + [int(word2id[word]) for word in text.split(' ')] + [0]\n    #\u5c06\u77ed\u53e5\u5b50\u6269\u5145\u5230\u7edf\u4e00\u7684\u957f\u5ea6\n    num = MAX_SEQ_LEN + 1 - len(data)\n    if(num >= 0):\n        data += [0]*num\n    else:\n        data = data[:MAX_SEQ_LEN] + [0]\n    return data\n\ndef wordDecode(arr, id2word):\n    out = []\n    for x in arr:\n        if x == 0:\n            break\n        out.append(id2word[x])\n    return ' '.join(out)", "metadata": {"trusted": true}, "execution_count": 6, "outputs": []}, {"cell_type": "code", "source": "class DataProvision:\n    def __init__(self, ques, ans, feat):\n        self.feat = feat\n        self.ques = ques\n        self.ans = ans\n\n    def __getitem__(self, index):\n#         print(np.array(self.feat[index], dtype=np.float32).shape)\n        return np.array(self.ques[index][1:], dtype=np.int32), \\\n               np.array(self.ans[index][:-1], dtype=np.int32), \\\n               np.array(self.feat[index], dtype=np.float32), \\\n               np.array(self.ans[index][1:], dtype=np.int32)\n\n    def __len__(self):\n        return len(self.ques)\n\ntrain_questions, train_answers, train_image_feature, val_questions, val_question_ids, val_image_feature, val_answers = preProcess(cfg)\nid2word, word2id = prepareVocab(train_answers + train_questions + val_questions + val_answers)\ntrain_questions = np.array([wordEncode(ques, word2id) for ques in train_questions])\ntrain_answers = np.array([wordEncode(ans, word2id) for ans in train_answers])\nprovider = DataProvision(train_questions, train_answers, train_image_feature)\nds_train = ds.GeneratorDataset(source=provider, column_names=['src', 'dst', 'feat', 'label'])\nds_train = ds_train.batch(cfg.batch_size)\nprint(ds_train.get_dataset_size())", "metadata": {"trusted": true}, "execution_count": 7, "outputs": [{"name": "stdout", "text": "loading VQA annotations and questions into memory...\n0:00:01.328212\ncreating index...\nindex created!\nfinished processing 0 in train\nfinished processing 1000 in train\nfinished processing 2000 in train\nfinished processing 3000 in train\nfinished processing 4000 in train\nfinished processing train\nloading VQA annotations and questions into memory...\n0:00:00.735394\ncreating index...\nindex created!\nfinished processing 0 in train\nfinished processing 1000 in train\nfinished processing val\nfinished processing features. Miss Images:  1989\nFinish prepare Vocab. Size: 3055\n128\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "def gru_default_state(batch_size, input_size, hidden_size, num_layers=1, bidirectional=False):\n    '''Weight init for gru cell'''\n    stdv = 1 / math.sqrt(hidden_size)\n    weight_i = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (input_size, 3*hidden_size)).astype(np.float32)), \n                         name='weight_i')\n    weight_h = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (hidden_size, 3*hidden_size)).astype(np.float32)), \n                         name='weight_h')\n    bias_i = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_i')\n    bias_h = Parameter(Tensor(\n        np.random.uniform(-stdv, stdv, (3*hidden_size)).astype(np.float32)), name='bias_h')\n    return weight_i, weight_h, bias_i, bias_h\n\nclass GRU(nn.Cell):\n    def __init__(self, config, is_training=True):\n        super(GRU, self).__init__()\n        if is_training:\n            self.batch_size = config.batch_size\n        else:\n            self.batch_size = config.eval_batch_size\n        self.hidden_size = config.hidden_size\n        self.weight_i, self.weight_h, self.bias_i, self.bias_h = \\\n            gru_default_state(self.batch_size, self.hidden_size, self.hidden_size)\n        self.rnn = P.DynamicGRUV2()\n        self.cast = P.Cast()\n\n    def construct(self, x, hidden):\n        x = self.cast(x, mstype.float16)\n        y1, h1, _, _, _, _ = self.rnn(x, self.weight_i, self.weight_h, self.bias_i, self.bias_h, None, hidden)\n        return y1, h1\n    \nclass Attention(nn.Cell):\n    def __init__(self, config, is_training=True):\n        super(Attention, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attnq = nn.Dense(self.hidden_size, self.hidden_size)\n        self.attni = nn.Dense(self.hidden_size, self.hidden_size)\n        self.attnp = nn.Dense(self.hidden_size, 1, activation = \"softmax\")\n        self.add = P.Add()\n        self.mul = P.Mul()\n        self.tanh = nn.Tanh()\n        self.trans = P.Transpose()\n        self.perm = (1, 0, 2)\n\n    def construct(self, question, img):\n        i_attn = self.attni(img)\n        q_attn = self.attnq(question)\n        i_attn = self.trans(i_attn, self.perm)\n        i_attn = self.add(i_attn, q_attn)\n        i_attn = self.trans(i_attn, self.perm)\n        ha = self.tanh(i_attn)\n        p = self.attnp(ha)\n        u = self.mul(p,img).sum(axis=1)\n        u = self.add(u, question)\n        return u\n\nclass SAN(nn.Cell):\n    def __init__(self, config, is_training=True):\n        super(SAN, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.attn_1 = Attention(config = config, is_training = is_training)\n        self.attn_2 = Attention(config = config, is_training = is_training)\n        self.attn_3 = Attention(config = config, is_training = is_training)\n        self.trans = P.Transpose()\n        self.dense = nn.Dense(2048, config.hidden_size)\n        self.tanh = nn.Tanh()\n\n    def construct(self, question, img):\n        tmp = self.dense(img)\n        tmp = self.tanh(tmp)\n        u_1 = self.attn_1(question, tmp)\n        u_2 = self.attn_2(u_1, tmp)\n\n        return u_2\n        \nclass Encoder(nn.Cell):\n    def __init__(self, config, is_training=True):\n        super(Encoder, self).__init__()\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        if is_training:\n            self.batch_size = config.batch_size\n        else:\n            self.batch_size = config.eval_batch_size\n\n        self.trans = P.Transpose()\n        self.perm = (1, 0, 2)\n        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n        self.cast = P.Cast()\n\n    def construct(self, encoder_input):\n        embeddings = self.embedding(encoder_input)\n        embeddings = self.trans(embeddings, self.perm)\n        output, hidden = self.gru(embeddings, self.h)\n        return self.cast(output, mstype.float32), hidden\n\nclass Decoder(nn.Cell):\n    def __init__(self, config, is_training=True, dropout=0.1):\n        super(Decoder, self).__init__()\n\n        self.vocab_size = config.vocab_size\n        self.hidden_size = config.hidden_size\n        self.max_len = config.max_seq_length\n\n        self.trans = P.Transpose()\n        self.perm = (1, 0, 2)\n        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n        self.dropout = nn.Dropout(1-dropout)\n        self.attn = nn.Dense(self.hidden_size, self.max_len)\n        self.softmax = nn.Softmax(axis=2)\n        self.bmm = P.BatchMatMul()\n        self.concat = P.Concat(axis=2)\n        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n\n        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n        self.logsoftmax = nn.LogSoftmax(axis=2)\n        self.cast = P.Cast()\n\n    def construct(self, decoder_input, hidden, encoder_output):\n        embeddings = self.embedding(decoder_input)\n        embeddings = self.dropout(embeddings)\n\n        embeddings = self.trans(embeddings, self.perm)\n        output, hidden = self.gru(embeddings, hidden)\n        output = self.cast(output, mstype.float32)\n        output = self.out(output)\n        output = self.logsoftmax(output)\n\n        return output, hidden\n\nclass Seq2Seq(nn.Cell):\n    def __init__(self, config, is_train=True):\n        super(Seq2Seq, self).__init__()\n        self.max_len = config.max_seq_length\n        self.is_train = is_train\n\n        self.encoder = Encoder(config, is_train)\n        self.decoder = Decoder(config, is_train)\n        self.expanddims = P.ExpandDims()\n        self.squeeze = P.Squeeze(axis=0)\n        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n        self.concat = P.Concat(axis=1)\n        self.concat2 = P.Concat(axis=0)\n        self.select = P.Select()\n        self.san = SAN(config,is_train)\n\n    def construct(self, src, dst, img):\n        encoder_output, hidden = self.encoder(src)\n        san_out = self.san(encoder_output[-1], img)\n        \n        decoder_hidden = san_out\n        if self.is_train:\n            outputs, _ = self.decoder(dst, decoder_hidden, san_out)\n        else:\n            decoder_input = dst[::,0:1:1]\n            decoder_outputs = ()\n            for i in range(0, self.max_len):\n                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, san_out)\n                decoder_hidden = self.squeeze(decoder_hidden)\n                decoder_output, _ = self.argmax(decoder_output)\n                decoder_output = self.squeeze(decoder_output)\n                decoder_outputs += (decoder_output,)\n                decoder_input = decoder_output\n            outputs = self.concat(decoder_outputs)\n        return outputs\n\nclass NLLLoss(_Loss):\n    '''\n       NLLLoss function\n    '''\n    def __init__(self, reduction='mean'):\n        super(NLLLoss, self).__init__(reduction)\n        self.one_hot = P.OneHot()\n        self.reduce_sum = P.ReduceSum()\n\n    def construct(self, logits, label):\n        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n                                     F.scalar_to_array(0.0))\n        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n        return self.get_loss(loss)\n    \nclass WithLossCell(nn.Cell):\n    def __init__(self, backbone, config):\n        super(WithLossCell, self).__init__(auto_prefix=False)\n        self._backbone = backbone\n        self.batch_size = config.batch_size\n        self.onehot = nn.OneHot(depth=config.vocab_size)\n        self._loss_fn = NLLLoss()\n        self.max_len = config.max_seq_length\n        self.squeeze = P.Squeeze()\n        self.cast = P.Cast()\n        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n        self.print = P.Print()\n\n    def construct(self, src, dst, feat, label):\n        out = self._backbone(src, dst, feat)\n        loss_total = 0\n        for i in range(self.batch_size):\n            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n                                 self.squeeze(label[i:i+1:1, ::]))\n            loss_total += loss\n        loss = loss_total / self.batch_size\n        return loss", "metadata": {"trusted": true}, "execution_count": 8, "outputs": []}, {"cell_type": "code", "source": "network = Seq2Seq(cfg)\nnetwork = WithLossCell(network, cfg)\noptimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\nmodel = Model(network, optimizer=optimizer)", "metadata": {"trusted": true}, "execution_count": 9, "outputs": [{"name": "stderr", "text": "[WARNING] ME(21453:281473602547296,MainProcess):2022-06-27-14:26:30.876.418 [mindspore/nn/loss/loss.py:103] '_Loss' is deprecated from version 1.3 and will be removed in a future version, use 'LossBase' instead.\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "loss_cb = LossMonitor()\ntime_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\ncallbacks = [time_cb, loss_cb]\n\nmodel.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=False)\nsave_checkpoint(model.train_network, cfg.checkpointFile)\nmox.file.copy_parallel(src_url=cfg.checkpointFile, dst_url='s3://nlp---3190101095/nlp/san.ckpt')", "metadata": {"scrolled": true, "trusted": true}, "execution_count": 10, "outputs": [{"name": "stdout", "text": "epoch: 1 step: 1, loss is 38.09493\nepoch: 1 step: 2, loss is 22.368027\nepoch: 1 step: 3, loss is 7.829624\nepoch: 1 step: 4, loss is 7.099731\nepoch: 1 step: 5, loss is 7.2276335\nepoch: 1 step: 6, loss is 10.766493\nepoch: 1 step: 7, loss is 5.778524\nepoch: 1 step: 8, loss is 6.500427\nepoch: 1 step: 9, loss is 5.923667\nepoch: 1 step: 10, loss is 10.6613035\nepoch: 1 step: 11, loss is 5.900267\nepoch: 1 step: 12, loss is 14.860427\nepoch: 1 step: 13, loss is 8.539547\nepoch: 1 step: 14, loss is 6.819925\nepoch: 1 step: 15, loss is 9.511694\nepoch: 1 step: 16, loss is 11.066891\nepoch: 1 step: 17, loss is 9.313786\nepoch: 1 step: 18, loss is 5.9087367\nepoch: 1 step: 19, loss is 14.609064\nepoch: 1 step: 20, loss is 13.766233\nepoch: 1 step: 21, loss is 19.339743\nepoch: 1 step: 22, loss is 9.903465\nepoch: 1 step: 23, loss is 6.839871\nepoch: 1 step: 24, loss is 9.718821\nepoch: 1 step: 25, loss is 8.96584\nepoch: 1 step: 26, loss is 10.745544\nepoch: 1 step: 27, loss is 10.298734\nepoch: 1 step: 28, loss is 5.869674\nepoch: 1 step: 29, loss is 13.307151\nepoch: 1 step: 30, loss is 9.256918\nepoch: 1 step: 31, loss is 9.431365\nepoch: 1 step: 32, loss is 9.794625\nepoch: 1 step: 33, loss is 13.551421\nepoch: 1 step: 34, loss is 6.2971597\nepoch: 1 step: 35, loss is 10.018934\nepoch: 1 step: 36, loss is 7.1694446\nepoch: 1 step: 37, loss is 16.39249\nepoch: 1 step: 38, loss is 13.849441\nepoch: 1 step: 39, loss is 7.3010025\nepoch: 1 step: 40, loss is 7.994965\nepoch: 1 step: 41, loss is 9.030429\nepoch: 1 step: 42, loss is 11.407452\nepoch: 1 step: 43, loss is 5.0947146\nepoch: 1 step: 44, loss is 9.602373\nepoch: 1 step: 45, loss is 15.413676\nepoch: 1 step: 46, loss is 11.527494\nepoch: 1 step: 47, loss is 12.967804\nepoch: 1 step: 48, loss is 9.710531\nepoch: 1 step: 49, loss is 8.810873\nepoch: 1 step: 50, loss is 13.07858\nepoch: 1 step: 51, loss is 7.8989105\nepoch: 1 step: 52, loss is 14.723482\nepoch: 1 step: 53, loss is 10.461462\nepoch: 1 step: 54, loss is 11.553675\nepoch: 1 step: 55, loss is 13.214907\nepoch: 1 step: 56, loss is 17.485025\nepoch: 1 step: 57, loss is 8.274748\nepoch: 1 step: 58, loss is 12.905463\nepoch: 1 step: 59, loss is 12.720372\nepoch: 1 step: 60, loss is 8.092975\nepoch: 1 step: 61, loss is 8.908404\nepoch: 1 step: 62, loss is 8.53258\nepoch: 1 step: 63, loss is 10.668395\nepoch: 1 step: 64, loss is 15.228987\nepoch: 1 step: 65, loss is 6.506423\nepoch: 1 step: 66, loss is 11.7301855\nepoch: 1 step: 67, loss is 9.191895\nepoch: 1 step: 68, loss is 4.1764007\nepoch: 1 step: 69, loss is 10.5424185\nepoch: 1 step: 70, loss is 10.322432\nepoch: 1 step: 71, loss is 14.847377\nepoch: 1 step: 72, loss is 14.339048\nepoch: 1 step: 73, loss is 12.566794\nepoch: 1 step: 74, loss is 12.841641\nepoch: 1 step: 75, loss is 12.867919\nepoch: 1 step: 76, loss is 11.4420595\nepoch: 1 step: 77, loss is 16.210299\nepoch: 1 step: 78, loss is 12.486253\nepoch: 1 step: 79, loss is 15.855388\nepoch: 1 step: 80, loss is 16.304758\nepoch: 1 step: 81, loss is 6.208242\nepoch: 1 step: 82, loss is 14.935228\nepoch: 1 step: 83, loss is 16.2417\nepoch: 1 step: 84, loss is 12.08518\nepoch: 1 step: 85, loss is 10.134944\nepoch: 1 step: 86, loss is 17.439243\nepoch: 1 step: 87, loss is 7.7191668\nepoch: 1 step: 88, loss is 13.420231\nepoch: 1 step: 89, loss is 16.504189\nepoch: 1 step: 90, loss is 11.954853\nepoch: 1 step: 91, loss is 14.211141\nepoch: 1 step: 92, loss is 12.553434\nepoch: 1 step: 93, loss is 16.681679\nepoch: 1 step: 94, loss is 14.567315\nepoch: 1 step: 95, loss is 10.144827\nepoch: 1 step: 96, loss is 13.252944\nepoch: 1 step: 97, loss is 14.527083\nepoch: 1 step: 98, loss is 12.107313\nepoch: 1 step: 99, loss is 7.754603\nepoch: 1 step: 100, loss is 18.54557\nepoch: 1 step: 101, loss is 11.198244\nepoch: 1 step: 102, loss is 8.439176\nepoch: 1 step: 103, loss is 15.884411\nepoch: 1 step: 104, loss is 18.094238\nepoch: 1 step: 105, loss is 5.3620467\nepoch: 1 step: 106, loss is 13.752492\nepoch: 1 step: 107, loss is 17.954233\nepoch: 1 step: 108, loss is 12.661736\nepoch: 1 step: 109, loss is 10.428939\nepoch: 1 step: 110, loss is 11.620331\nepoch: 1 step: 111, loss is 17.908283\nepoch: 1 step: 112, loss is 15.882809\nepoch: 1 step: 113, loss is 20.056112\nepoch: 1 step: 114, loss is 16.832716\nepoch: 1 step: 115, loss is 10.86672\nepoch: 1 step: 116, loss is 18.037403\nepoch: 1 step: 117, loss is 11.131206\nepoch: 1 step: 118, loss is 9.821729\nepoch: 1 step: 119, loss is 16.773632\nepoch: 1 step: 120, loss is 16.514349\nepoch: 1 step: 121, loss is 11.601448\nepoch: 1 step: 122, loss is 21.00755\nepoch: 1 step: 123, loss is 8.871606\nepoch: 1 step: 124, loss is 12.359054\nepoch: 1 step: 125, loss is 13.086535\nepoch: 1 step: 126, loss is 10.884587\nepoch: 1 step: 127, loss is 14.601596\nepoch: 1 step: 128, loss is 10.000451\nepoch time: 103928.299 ms, per step time: 811.940 ms\nepoch: 2 step: 1, loss is 12.34996\nepoch: 2 step: 2, loss is 13.721209\nepoch: 2 step: 3, loss is 15.31118\nepoch: 2 step: 4, loss is 13.371855\nepoch: 2 step: 5, loss is 18.620134\nepoch: 2 step: 6, loss is 9.969214\nepoch: 2 step: 7, loss is 11.504009\nepoch: 2 step: 8, loss is 5.6345067\nepoch: 2 step: 9, loss is 16.20294\nepoch: 2 step: 10, loss is 11.925042\nepoch: 2 step: 11, loss is 12.573795\nepoch: 2 step: 12, loss is 14.078305\nepoch: 2 step: 13, loss is 12.504115\nepoch: 2 step: 14, loss is 9.471037\nepoch: 2 step: 15, loss is 9.888675\nepoch: 2 step: 16, loss is 12.311702\nepoch: 2 step: 17, loss is 8.851686\nepoch: 2 step: 18, loss is 14.048655\nepoch: 2 step: 19, loss is 14.762121\nepoch: 2 step: 20, loss is 13.805096\nepoch: 2 step: 21, loss is 9.774401\nepoch: 2 step: 22, loss is 9.837242\nepoch: 2 step: 23, loss is 12.377388\nepoch: 2 step: 24, loss is 7.487351\nepoch: 2 step: 25, loss is 13.210856\nepoch: 2 step: 26, loss is 17.797396\nepoch: 2 step: 27, loss is 16.261917\nepoch: 2 step: 28, loss is 11.473496\nepoch: 2 step: 29, loss is 11.252049\nepoch: 2 step: 30, loss is 13.259724\nepoch: 2 step: 31, loss is 12.765292\nepoch: 2 step: 32, loss is 12.113018\nepoch: 2 step: 33, loss is 13.20513\nepoch: 2 step: 34, loss is 7.89942\nepoch: 2 step: 35, loss is 8.727245\nepoch: 2 step: 36, loss is 17.560003\nepoch: 2 step: 37, loss is 11.352656\nepoch: 2 step: 38, loss is 9.865385\nepoch: 2 step: 39, loss is 11.650492\nepoch: 2 step: 40, loss is 8.844287\nepoch: 2 step: 41, loss is 12.913465\nepoch: 2 step: 42, loss is 10.4881315\nepoch: 2 step: 43, loss is 9.2367735\nepoch: 2 step: 44, loss is 11.813885\nepoch: 2 step: 45, loss is 6.5568676\nepoch: 2 step: 46, loss is 15.468998\nepoch: 2 step: 47, loss is 13.780972\nepoch: 2 step: 48, loss is 9.720331\nepoch: 2 step: 49, loss is 8.911005\nepoch: 2 step: 50, loss is 9.414101\nepoch: 2 step: 51, loss is 13.798243\nepoch: 2 step: 52, loss is 9.765345\nepoch: 2 step: 53, loss is 12.238462\nepoch: 2 step: 54, loss is 13.200348\nepoch: 2 step: 55, loss is 12.595621\nepoch: 2 step: 56, loss is 18.396978\nepoch: 2 step: 57, loss is 15.410886\nepoch: 2 step: 58, loss is 11.070171\nepoch: 2 step: 59, loss is 17.777967\nepoch: 2 step: 60, loss is 11.240244\nepoch: 2 step: 61, loss is 19.176973\nepoch: 2 step: 62, loss is 8.468948\nepoch: 2 step: 63, loss is 10.969868\nepoch: 2 step: 64, loss is 8.552532\nepoch: 2 step: 65, loss is 11.852862\nepoch: 2 step: 66, loss is 21.875021\nepoch: 2 step: 67, loss is 7.8473797\nepoch: 2 step: 68, loss is 14.803578\nepoch: 2 step: 69, loss is 10.334048\nepoch: 2 step: 70, loss is 6.9408445\nepoch: 2 step: 71, loss is 6.8907647\nepoch: 2 step: 72, loss is 16.33506\nepoch: 2 step: 73, loss is 13.7232065\nepoch: 2 step: 74, loss is 12.122858\nepoch: 2 step: 75, loss is 17.66936\nepoch: 2 step: 76, loss is 10.6737795\nepoch: 2 step: 77, loss is 14.811133\nepoch: 2 step: 78, loss is 10.981305\nepoch: 2 step: 79, loss is 10.898382\nepoch: 2 step: 80, loss is 13.184459\nepoch: 2 step: 81, loss is 9.108839\nepoch: 2 step: 82, loss is 4.6588225\nepoch: 2 step: 83, loss is 9.062018\nepoch: 2 step: 84, loss is 19.300898\nepoch: 2 step: 85, loss is 8.073918\nepoch: 2 step: 86, loss is 19.651031\nepoch: 2 step: 87, loss is 22.424377\nepoch: 2 step: 88, loss is 8.538897\nepoch: 2 step: 89, loss is 13.157803\nepoch: 2 step: 90, loss is 15.390869\nepoch: 2 step: 91, loss is 9.747786\nepoch: 2 step: 92, loss is 11.699746\nepoch: 2 step: 93, loss is 9.815719\nepoch: 2 step: 94, loss is 9.687358\nepoch: 2 step: 95, loss is 5.6760173\nepoch: 2 step: 96, loss is 14.076407\nepoch: 2 step: 97, loss is 19.281982\nepoch: 2 step: 98, loss is 14.814995\nepoch: 2 step: 99, loss is 15.1026325\nepoch: 2 step: 100, loss is 12.768712\nepoch: 2 step: 101, loss is 14.401803\nepoch: 2 step: 102, loss is 22.14481\nepoch: 2 step: 103, loss is 10.709191\nepoch: 2 step: 104, loss is 17.190302\nepoch: 2 step: 105, loss is 6.8524456\nepoch: 2 step: 106, loss is 24.803791\nepoch: 2 step: 107, loss is 11.257382\nepoch: 2 step: 108, loss is 6.3323708\nepoch: 2 step: 109, loss is 16.795809\nepoch: 2 step: 110, loss is 22.940481\nepoch: 2 step: 111, loss is 17.784737\nepoch: 2 step: 112, loss is 12.048128\nepoch: 2 step: 113, loss is 11.420298\nepoch: 2 step: 114, loss is 8.508368\nepoch: 2 step: 115, loss is 7.4861903\nepoch: 2 step: 116, loss is 20.282928\nepoch: 2 step: 117, loss is 11.45844\nepoch: 2 step: 118, loss is 11.797026\nepoch: 2 step: 119, loss is 11.700494\nepoch: 2 step: 120, loss is 8.897867\nepoch: 2 step: 121, loss is 23.75592\nepoch: 2 step: 122, loss is 14.9719925\nepoch: 2 step: 123, loss is 19.709244\nepoch: 2 step: 124, loss is 13.985597\nepoch: 2 step: 125, loss is 15.373442\nepoch: 2 step: 126, loss is 13.318586\nepoch: 2 step: 127, loss is 14.7973175\nepoch: 2 step: 128, loss is 13.569799\nepoch time: 43818.180 ms, per step time: 342.330 ms\nepoch: 3 step: 1, loss is 12.849273\nepoch: 3 step: 2, loss is 7.930118\nepoch: 3 step: 3, loss is 12.284123\nepoch: 3 step: 4, loss is 9.57992\nepoch: 3 step: 5, loss is 13.673652\nepoch: 3 step: 6, loss is 8.523436\nepoch: 3 step: 7, loss is 4.777693\nepoch: 3 step: 8, loss is 11.986972\nepoch: 3 step: 9, loss is 9.795631\nepoch: 3 step: 10, loss is 7.019174\nepoch: 3 step: 11, loss is 9.6932535\nepoch: 3 step: 12, loss is 15.222753\nepoch: 3 step: 13, loss is 9.816736\nepoch: 3 step: 14, loss is 13.209595\nepoch: 3 step: 15, loss is 8.6138\nepoch: 3 step: 16, loss is 7.8201356\nepoch: 3 step: 17, loss is 5.0910563\nepoch: 3 step: 18, loss is 8.127463\nepoch: 3 step: 19, loss is 9.901532\nepoch: 3 step: 20, loss is 14.124351\nepoch: 3 step: 21, loss is 6.078285\nepoch: 3 step: 22, loss is 15.759796\nepoch: 3 step: 23, loss is 10.94016\nepoch: 3 step: 24, loss is 14.055945\nepoch: 3 step: 25, loss is 13.0107\nepoch: 3 step: 26, loss is 8.701677\nepoch: 3 step: 27, loss is 17.368326\nepoch: 3 step: 28, loss is 12.850202\nepoch: 3 step: 29, loss is 11.878367\nepoch: 3 step: 30, loss is 14.680142\nepoch: 3 step: 31, loss is 8.167924\nepoch: 3 step: 32, loss is 12.553384\nepoch: 3 step: 33, loss is 20.538153\nepoch: 3 step: 34, loss is 9.772789\nepoch: 3 step: 35, loss is 10.76333\nepoch: 3 step: 36, loss is 11.20667\nepoch: 3 step: 37, loss is 16.900019\nepoch: 3 step: 38, loss is 12.356987\nepoch: 3 step: 39, loss is 13.471974\nepoch: 3 step: 40, loss is 9.376452\nepoch: 3 step: 41, loss is 12.518097\nepoch: 3 step: 42, loss is 13.449446\nepoch: 3 step: 43, loss is 10.601018\nepoch: 3 step: 44, loss is 13.125086\nepoch: 3 step: 45, loss is 9.342725\nepoch: 3 step: 46, loss is 13.284591\nepoch: 3 step: 47, loss is 15.016846\nepoch: 3 step: 48, loss is 12.845637\nepoch: 3 step: 49, loss is 11.720717\nepoch: 3 step: 50, loss is 9.928086\nepoch: 3 step: 51, loss is 11.311053\nepoch: 3 step: 52, loss is 10.3708725\nepoch: 3 step: 53, loss is 17.812496\nepoch: 3 step: 54, loss is 14.402655\nepoch: 3 step: 55, loss is 11.764322\nepoch: 3 step: 56, loss is 12.378055\nepoch: 3 step: 57, loss is 15.542288\nepoch: 3 step: 58, loss is 14.202534\nepoch: 3 step: 59, loss is 10.894976\nepoch: 3 step: 60, loss is 8.878392\nepoch: 3 step: 61, loss is 11.357279\nepoch: 3 step: 62, loss is 10.961956\nepoch: 3 step: 63, loss is 11.551586\nepoch: 3 step: 64, loss is 10.518486\nepoch: 3 step: 65, loss is 13.8403635\nepoch: 3 step: 66, loss is 10.988664\nepoch: 3 step: 67, loss is 14.16349\nepoch: 3 step: 68, loss is 9.314478\nepoch: 3 step: 69, loss is 7.0053782\nepoch: 3 step: 70, loss is 12.168395\nepoch: 3 step: 71, loss is 13.181847\nepoch: 3 step: 72, loss is 23.62431\nepoch: 3 step: 73, loss is 12.955419\nepoch: 3 step: 74, loss is 8.703113\nepoch: 3 step: 75, loss is 16.474928\nepoch: 3 step: 76, loss is 8.284006\nepoch: 3 step: 77, loss is 15.290428\nepoch: 3 step: 78, loss is 10.021613\nepoch: 3 step: 79, loss is 18.692461\nepoch: 3 step: 80, loss is 15.20608\nepoch: 3 step: 81, loss is 13.887741\nepoch: 3 step: 82, loss is 12.550639\nepoch: 3 step: 83, loss is 11.9252205\nepoch: 3 step: 84, loss is 10.754179\nepoch: 3 step: 85, loss is 9.898889\nepoch: 3 step: 86, loss is 6.5522695\nepoch: 3 step: 87, loss is 10.729791\nepoch: 3 step: 88, loss is 6.6068544\nepoch: 3 step: 89, loss is 6.7236204\nepoch: 3 step: 90, loss is 7.7258673\nepoch: 3 step: 91, loss is 5.0620284\nepoch: 3 step: 92, loss is 6.2607465\nepoch: 3 step: 93, loss is 10.120833\nepoch: 3 step: 94, loss is 11.611454\nepoch: 3 step: 95, loss is 16.821777\nepoch: 3 step: 96, loss is 9.03661\nepoch: 3 step: 97, loss is 9.285746\nepoch: 3 step: 98, loss is 13.273836\nepoch: 3 step: 99, loss is 14.56586\nepoch: 3 step: 100, loss is 9.269684\nepoch: 3 step: 101, loss is 7.307082\nepoch: 3 step: 102, loss is 14.935694\nepoch: 3 step: 103, loss is 12.632642\nepoch: 3 step: 104, loss is 7.8356647\nepoch: 3 step: 105, loss is 10.901011\nepoch: 3 step: 106, loss is 22.59102\nepoch: 3 step: 107, loss is 11.329079\nepoch: 3 step: 108, loss is 19.085829\nepoch: 3 step: 109, loss is 6.278543\nepoch: 3 step: 110, loss is 5.5309753\nepoch: 3 step: 111, loss is 6.671241\nepoch: 3 step: 112, loss is 12.126793\nepoch: 3 step: 113, loss is 11.177587\nepoch: 3 step: 114, loss is 13.62698\nepoch: 3 step: 115, loss is 7.475011\nepoch: 3 step: 116, loss is 15.489296\nepoch: 3 step: 117, loss is 11.242875\nepoch: 3 step: 118, loss is 9.576834\nepoch: 3 step: 119, loss is 14.384092\nepoch: 3 step: 120, loss is 7.546751\nepoch: 3 step: 121, loss is 11.6173315\nepoch: 3 step: 122, loss is 8.926912\nepoch: 3 step: 123, loss is 8.045273\nepoch: 3 step: 124, loss is 10.825034\nepoch: 3 step: 125, loss is 11.004369\nepoch: 3 step: 126, loss is 8.88553\nepoch: 3 step: 127, loss is 9.596935\nepoch: 3 step: 128, loss is 11.167472\nepoch time: 43828.411 ms, per step time: 342.409 ms\nepoch: 4 step: 1, loss is 9.646553\nepoch: 4 step: 2, loss is 7.043014\nepoch: 4 step: 3, loss is 7.8262296\nepoch: 4 step: 4, loss is 4.891796\nepoch: 4 step: 5, loss is 3.01028\nepoch: 4 step: 6, loss is 7.4362793\nepoch: 4 step: 7, loss is 5.4342165\nepoch: 4 step: 8, loss is 6.773735\nepoch: 4 step: 9, loss is 4.44703\nepoch: 4 step: 10, loss is 5.1917696\nepoch: 4 step: 11, loss is 10.500391\nepoch: 4 step: 12, loss is 4.1347632\nepoch: 4 step: 13, loss is 9.202141\nepoch: 4 step: 14, loss is 7.5341234\nepoch: 4 step: 15, loss is 6.844608\nepoch: 4 step: 16, loss is 8.848217\nepoch: 4 step: 17, loss is 9.827616\nepoch: 4 step: 18, loss is 3.9381094\nepoch: 4 step: 19, loss is 7.6400013\nepoch: 4 step: 20, loss is 6.7236953\nepoch: 4 step: 21, loss is 6.0949545\nepoch: 4 step: 22, loss is 8.307098\nepoch: 4 step: 23, loss is 4.4258423\nepoch: 4 step: 24, loss is 4.818684\nepoch: 4 step: 25, loss is 7.2857924\nepoch: 4 step: 26, loss is 7.397859\nepoch: 4 step: 27, loss is 5.450081\nepoch: 4 step: 28, loss is 7.023584\nepoch: 4 step: 29, loss is 7.1058536\nepoch: 4 step: 30, loss is 5.2120714\nepoch: 4 step: 31, loss is 7.0553584\nepoch: 4 step: 32, loss is 7.781282\nepoch: 4 step: 33, loss is 7.7902055\nepoch: 4 step: 34, loss is 4.9045663\nepoch: 4 step: 35, loss is 6.8548265\nepoch: 4 step: 36, loss is 6.0966907\nepoch: 4 step: 37, loss is 8.662098\nepoch: 4 step: 38, loss is 4.678613\nepoch: 4 step: 39, loss is 9.3566475\nepoch: 4 step: 40, loss is 14.745336\nepoch: 4 step: 41, loss is 10.551398\nepoch: 4 step: 42, loss is 8.506538\nepoch: 4 step: 43, loss is 12.640532\nepoch: 4 step: 44, loss is 11.775036\nepoch: 4 step: 45, loss is 8.519172\nepoch: 4 step: 46, loss is 17.604538\nepoch: 4 step: 47, loss is 10.227076\nepoch: 4 step: 48, loss is 11.65328\nepoch: 4 step: 49, loss is 13.616093\nepoch: 4 step: 50, loss is 7.624482\nepoch: 4 step: 51, loss is 7.1119747\nepoch: 4 step: 52, loss is 6.7773466\nepoch: 4 step: 53, loss is 14.040735\nepoch: 4 step: 54, loss is 9.488646\nepoch: 4 step: 55, loss is 5.109953\nepoch: 4 step: 56, loss is 10.652592\nepoch: 4 step: 57, loss is 7.8396325\nepoch: 4 step: 58, loss is 11.443152\nepoch: 4 step: 59, loss is 8.825337\nepoch: 4 step: 60, loss is 10.005891\nepoch: 4 step: 61, loss is 7.2669287\nepoch: 4 step: 62, loss is 12.352471\nepoch: 4 step: 63, loss is 6.397873\nepoch: 4 step: 64, loss is 9.437018\nepoch: 4 step: 65, loss is 5.727305\nepoch: 4 step: 66, loss is 5.6887054\nepoch: 4 step: 67, loss is 6.9671783\nepoch: 4 step: 68, loss is 6.891716\nepoch: 4 step: 69, loss is 8.092316\nepoch: 4 step: 70, loss is 8.750739\nepoch: 4 step: 71, loss is 10.279394\nepoch: 4 step: 72, loss is 8.087445\nepoch: 4 step: 73, loss is 7.5673323\nepoch: 4 step: 74, loss is 5.581632\nepoch: 4 step: 75, loss is 8.675964\nepoch: 4 step: 76, loss is 10.421781\nepoch: 4 step: 77, loss is 10.996547\nepoch: 4 step: 78, loss is 8.022292\nepoch: 4 step: 79, loss is 11.0191965\nepoch: 4 step: 80, loss is 15.965453\nepoch: 4 step: 81, loss is 8.142953\nepoch: 4 step: 82, loss is 11.643088\nepoch: 4 step: 83, loss is 13.835321\nepoch: 4 step: 84, loss is 6.835736\nepoch: 4 step: 85, loss is 8.670293\nepoch: 4 step: 86, loss is 7.655565\nepoch: 4 step: 87, loss is 10.197556\nepoch: 4 step: 88, loss is 5.3887806\nepoch: 4 step: 89, loss is 10.4873\nepoch: 4 step: 90, loss is 7.19013\nepoch: 4 step: 91, loss is 14.322959\nepoch: 4 step: 92, loss is 11.147624\nepoch: 4 step: 93, loss is 6.3723607\nepoch: 4 step: 94, loss is 12.723537\nepoch: 4 step: 95, loss is 11.463774\nepoch: 4 step: 96, loss is 7.966362\nepoch: 4 step: 97, loss is 9.872641\nepoch: 4 step: 98, loss is 17.65708\nepoch: 4 step: 99, loss is 10.917742\nepoch: 4 step: 100, loss is 8.131535\nepoch: 4 step: 101, loss is 8.030065\nepoch: 4 step: 102, loss is 8.973112\nepoch: 4 step: 103, loss is 12.540367\nepoch: 4 step: 104, loss is 7.08733\nepoch: 4 step: 105, loss is 10.771568\nepoch: 4 step: 106, loss is 5.1444283\nepoch: 4 step: 107, loss is 6.6639223\nepoch: 4 step: 108, loss is 11.451971\nepoch: 4 step: 109, loss is 8.640024\nepoch: 4 step: 110, loss is 10.789795\nepoch: 4 step: 111, loss is 4.348426\nepoch: 4 step: 112, loss is 12.726254\nepoch: 4 step: 113, loss is 10.583891\nepoch: 4 step: 114, loss is 10.43748\nepoch: 4 step: 115, loss is 13.420023\nepoch: 4 step: 116, loss is 7.1974382\nepoch: 4 step: 117, loss is 11.928569\nepoch: 4 step: 118, loss is 11.009598\nepoch: 4 step: 119, loss is 4.874497\nepoch: 4 step: 120, loss is 12.827475\nepoch: 4 step: 121, loss is 10.543203\nepoch: 4 step: 122, loss is 11.382631\nepoch: 4 step: 123, loss is 7.9871445\nepoch: 4 step: 124, loss is 7.088865\nepoch: 4 step: 125, loss is 9.56629\nepoch: 4 step: 126, loss is 10.44502\nepoch: 4 step: 127, loss is 8.773341\nepoch: 4 step: 128, loss is 10.316116\nepoch time: 43804.703 ms, per step time: 342.224 ms\nepoch: 5 step: 1, loss is 8.360606\nepoch: 5 step: 2, loss is 9.076254\nepoch: 5 step: 3, loss is 4.56564\nepoch: 5 step: 4, loss is 8.582589\nepoch: 5 step: 5, loss is 5.970608\nepoch: 5 step: 6, loss is 8.232887\nepoch: 5 step: 7, loss is 14.135253\nepoch: 5 step: 8, loss is 6.5297747\nepoch: 5 step: 9, loss is 10.007799\nepoch: 5 step: 10, loss is 5.9431667\nepoch: 5 step: 11, loss is 8.370132\nepoch: 5 step: 12, loss is 5.050842\nepoch: 5 step: 13, loss is 4.848585\nepoch: 5 step: 14, loss is 5.574543\nepoch: 5 step: 15, loss is 9.670765\nepoch: 5 step: 16, loss is 8.608891\nepoch: 5 step: 17, loss is 6.723795\nepoch: 5 step: 18, loss is 4.2441826\nepoch: 5 step: 19, loss is 5.776345\nepoch: 5 step: 20, loss is 8.135862\nepoch: 5 step: 21, loss is 9.035907\nepoch: 5 step: 22, loss is 7.1049843\nepoch: 5 step: 23, loss is 13.21491\nepoch: 5 step: 24, loss is 6.2983046\nepoch: 5 step: 25, loss is 8.49189\nepoch: 5 step: 26, loss is 6.617799\nepoch: 5 step: 27, loss is 7.003916\nepoch: 5 step: 28, loss is 9.072047\nepoch: 5 step: 29, loss is 7.857859\nepoch: 5 step: 30, loss is 5.88794\nepoch: 5 step: 31, loss is 11.474638\nepoch: 5 step: 32, loss is 9.253785\nepoch: 5 step: 33, loss is 5.2799416\nepoch: 5 step: 34, loss is 8.428986\nepoch: 5 step: 35, loss is 7.8326993\nepoch: 5 step: 36, loss is 1.8065301\nepoch: 5 step: 37, loss is 7.4923677\nepoch: 5 step: 38, loss is 5.376337\nepoch: 5 step: 39, loss is 5.250679\nepoch: 5 step: 40, loss is 4.9160624\nepoch: 5 step: 41, loss is 13.578652\nepoch: 5 step: 42, loss is 6.6732893\nepoch: 5 step: 43, loss is 8.790686\nepoch: 5 step: 44, loss is 12.572014\nepoch: 5 step: 45, loss is 12.050351\nepoch: 5 step: 46, loss is 9.103768\nepoch: 5 step: 47, loss is 14.25688\nepoch: 5 step: 48, loss is 11.590271\nepoch: 5 step: 49, loss is 7.4847555\nepoch: 5 step: 50, loss is 3.7778525\nepoch: 5 step: 51, loss is 8.564951\nepoch: 5 step: 52, loss is 5.6191306\nepoch: 5 step: 53, loss is 10.335733\nepoch: 5 step: 54, loss is 7.8724523\nepoch: 5 step: 55, loss is 15.171385\nepoch: 5 step: 56, loss is 11.184665\nepoch: 5 step: 57, loss is 8.146146\nepoch: 5 step: 58, loss is 7.1115026\nepoch: 5 step: 59, loss is 8.730449\nepoch: 5 step: 60, loss is 7.015543\nepoch: 5 step: 61, loss is 6.521116\nepoch: 5 step: 62, loss is 9.71352\nepoch: 5 step: 63, loss is 7.556103\nepoch: 5 step: 64, loss is 14.386025\nepoch: 5 step: 65, loss is 6.438127\nepoch: 5 step: 66, loss is 6.4357285\nepoch: 5 step: 67, loss is 11.036774\nepoch: 5 step: 68, loss is 7.4445944\nepoch: 5 step: 69, loss is 11.532248\nepoch: 5 step: 70, loss is 7.9730253\nepoch: 5 step: 71, loss is 6.0023212\nepoch: 5 step: 72, loss is 8.579179\nepoch: 5 step: 73, loss is 9.778514\nepoch: 5 step: 74, loss is 9.024971\nepoch: 5 step: 75, loss is 4.726017\nepoch: 5 step: 76, loss is 7.394976\nepoch: 5 step: 77, loss is 5.8058305\nepoch: 5 step: 78, loss is 11.118823\nepoch: 5 step: 79, loss is 9.557286\nepoch: 5 step: 80, loss is 8.754614\nepoch: 5 step: 81, loss is 9.540594\nepoch: 5 step: 82, loss is 7.9694777\nepoch: 5 step: 83, loss is 9.135058\nepoch: 5 step: 84, loss is 8.461995\nepoch: 5 step: 85, loss is 16.112762\nepoch: 5 step: 86, loss is 13.8569975\nepoch: 5 step: 87, loss is 11.131679\nepoch: 5 step: 88, loss is 10.826945\nepoch: 5 step: 89, loss is 12.3041725\nepoch: 5 step: 90, loss is 10.453681\nepoch: 5 step: 91, loss is 10.500671\nepoch: 5 step: 92, loss is 9.474958\nepoch: 5 step: 93, loss is 5.625892\nepoch: 5 step: 94, loss is 11.0152025\nepoch: 5 step: 95, loss is 4.8449287\nepoch: 5 step: 96, loss is 6.0979285\nepoch: 5 step: 97, loss is 7.6582694\nepoch: 5 step: 98, loss is 10.698665\nepoch: 5 step: 99, loss is 5.3557024\nepoch: 5 step: 100, loss is 9.405292\nepoch: 5 step: 101, loss is 8.716496\nepoch: 5 step: 102, loss is 11.643864\nepoch: 5 step: 103, loss is 3.8320513\nepoch: 5 step: 104, loss is 4.214788\nepoch: 5 step: 105, loss is 14.010838\nepoch: 5 step: 106, loss is 11.466309\nepoch: 5 step: 107, loss is 14.528307\nepoch: 5 step: 108, loss is 11.32731\nepoch: 5 step: 109, loss is 8.927405\nepoch: 5 step: 110, loss is 7.1569505\nepoch: 5 step: 111, loss is 2.80863\nepoch: 5 step: 112, loss is 8.614041\nepoch: 5 step: 113, loss is 10.281126\nepoch: 5 step: 114, loss is 9.630327\nepoch: 5 step: 115, loss is 5.6903048\nepoch: 5 step: 116, loss is 16.384174\nepoch: 5 step: 117, loss is 4.9267035\nepoch: 5 step: 118, loss is 9.587103\nepoch: 5 step: 119, loss is 6.450361\nepoch: 5 step: 120, loss is 8.290336\nepoch: 5 step: 121, loss is 10.032627\nepoch: 5 step: 122, loss is 8.260086\nepoch: 5 step: 123, loss is 6.620364\nepoch: 5 step: 124, loss is 7.552057\nepoch: 5 step: 125, loss is 11.70411\nepoch: 5 step: 126, loss is 14.521572\nepoch: 5 step: 127, loss is 10.241312\nepoch: 5 step: 128, loss is 7.810152\nepoch time: 43811.228 ms, per step time: 342.275 ms\nepoch: 6 step: 1, loss is 8.027761\nepoch: 6 step: 2, loss is 9.558017\nepoch: 6 step: 3, loss is 4.2994847\nepoch: 6 step: 4, loss is 11.102171\nepoch: 6 step: 5, loss is 6.8038545\nepoch: 6 step: 6, loss is 6.4626904\nepoch: 6 step: 7, loss is 12.514295\nepoch: 6 step: 8, loss is 6.4473166\nepoch: 6 step: 9, loss is 10.642066\nepoch: 6 step: 10, loss is 12.391691\nepoch: 6 step: 11, loss is 3.7008386\nepoch: 6 step: 12, loss is 7.2609053\nepoch: 6 step: 13, loss is 9.4705105\nepoch: 6 step: 14, loss is 7.0654373\nepoch: 6 step: 15, loss is 7.647694\nepoch: 6 step: 16, loss is 11.163981\nepoch: 6 step: 17, loss is 3.791107\nepoch: 6 step: 18, loss is 9.872911\nepoch: 6 step: 19, loss is 9.316128\nepoch: 6 step: 20, loss is 7.5598426\nepoch: 6 step: 21, loss is 6.1811805\nepoch: 6 step: 22, loss is 11.094484\nepoch: 6 step: 23, loss is 9.483943\nepoch: 6 step: 24, loss is 8.126237\nepoch: 6 step: 25, loss is 8.039289\nepoch: 6 step: 26, loss is 9.32767\nepoch: 6 step: 27, loss is 7.3568854\nepoch: 6 step: 28, loss is 9.328502\nepoch: 6 step: 29, loss is 11.960605\nepoch: 6 step: 30, loss is 6.995085\nepoch: 6 step: 31, loss is 7.1802545\nepoch: 6 step: 32, loss is 11.268654\nepoch: 6 step: 33, loss is 4.372225\nepoch: 6 step: 34, loss is 8.24649\nepoch: 6 step: 35, loss is 10.291576\nepoch: 6 step: 36, loss is 7.3213964\nepoch: 6 step: 37, loss is 6.4845815\nepoch: 6 step: 38, loss is 5.341503\nepoch: 6 step: 39, loss is 10.568778\nepoch: 6 step: 40, loss is 7.830086\nepoch: 6 step: 41, loss is 15.649368\nepoch: 6 step: 42, loss is 6.9034505\nepoch: 6 step: 43, loss is 10.243543\nepoch: 6 step: 44, loss is 10.170685\nepoch: 6 step: 45, loss is 6.135534\nepoch: 6 step: 46, loss is 6.979438\nepoch: 6 step: 47, loss is 9.10431\nepoch: 6 step: 48, loss is 10.7839985\nepoch: 6 step: 49, loss is 6.323773\nepoch: 6 step: 50, loss is 10.936819\nepoch: 6 step: 51, loss is 8.618348\nepoch: 6 step: 52, loss is 8.639326\nepoch: 6 step: 53, loss is 7.3572702\nepoch: 6 step: 54, loss is 11.922981\nepoch: 6 step: 55, loss is 6.7541966\nepoch: 6 step: 56, loss is 7.459272\nepoch: 6 step: 57, loss is 8.532816\nepoch: 6 step: 58, loss is 7.147399\nepoch: 6 step: 59, loss is 13.144867\nepoch: 6 step: 60, loss is 10.697361\nepoch: 6 step: 61, loss is 13.795139\nepoch: 6 step: 62, loss is 10.03382\nepoch: 6 step: 63, loss is 7.8120713\nepoch: 6 step: 64, loss is 6.491258\nepoch: 6 step: 65, loss is 8.692753\nepoch: 6 step: 66, loss is 12.948309\nepoch: 6 step: 67, loss is 8.059844\nepoch: 6 step: 68, loss is 12.399261\nepoch: 6 step: 69, loss is 6.420445\nepoch: 6 step: 70, loss is 18.965258\nepoch: 6 step: 71, loss is 4.9973783\nepoch: 6 step: 72, loss is 6.882819\nepoch: 6 step: 73, loss is 12.134647\nepoch: 6 step: 74, loss is 9.086882\nepoch: 6 step: 75, loss is 10.918604\nepoch: 6 step: 76, loss is 8.814478\nepoch: 6 step: 77, loss is 8.09755\nepoch: 6 step: 78, loss is 17.23822\nepoch: 6 step: 79, loss is 6.8714914\nepoch: 6 step: 80, loss is 8.550852\nepoch: 6 step: 81, loss is 7.913466\nepoch: 6 step: 82, loss is 5.3086057\nepoch: 6 step: 83, loss is 8.798831\nepoch: 6 step: 84, loss is 9.297656\nepoch: 6 step: 85, loss is 7.520313\nepoch: 6 step: 86, loss is 12.874156\nepoch: 6 step: 87, loss is 11.080763\nepoch: 6 step: 88, loss is 10.068822\nepoch: 6 step: 89, loss is 7.7499924\nepoch: 6 step: 90, loss is 8.047803\nepoch: 6 step: 91, loss is 9.210792\nepoch: 6 step: 92, loss is 9.800489\nepoch: 6 step: 93, loss is 8.981217\nepoch: 6 step: 94, loss is 12.801962\nepoch: 6 step: 95, loss is 6.2338743\nepoch: 6 step: 96, loss is 10.987646\nepoch: 6 step: 97, loss is 10.350455\nepoch: 6 step: 98, loss is 7.8285446\nepoch: 6 step: 99, loss is 7.0700126\nepoch: 6 step: 100, loss is 9.925009\nepoch: 6 step: 101, loss is 8.627704\nepoch: 6 step: 102, loss is 8.764659\nepoch: 6 step: 103, loss is 10.165387\nepoch: 6 step: 104, loss is 12.939661\nepoch: 6 step: 105, loss is 12.205039\nepoch: 6 step: 106, loss is 6.076208\nepoch: 6 step: 107, loss is 15.6764\nepoch: 6 step: 108, loss is 5.0972853\nepoch: 6 step: 109, loss is 10.730993\nepoch: 6 step: 110, loss is 6.9224153\nepoch: 6 step: 111, loss is 5.407324\nepoch: 6 step: 112, loss is 9.594596\nepoch: 6 step: 113, loss is 8.077948\nepoch: 6 step: 114, loss is 5.853047\nepoch: 6 step: 115, loss is 7.436605\nepoch: 6 step: 116, loss is 11.608446\nepoch: 6 step: 117, loss is 2.5433068\nepoch: 6 step: 118, loss is 5.818493\nepoch: 6 step: 119, loss is 8.892126\nepoch: 6 step: 120, loss is 9.057176\nepoch: 6 step: 121, loss is 5.188285\nepoch: 6 step: 122, loss is 11.86273\nepoch: 6 step: 123, loss is 7.142628\nepoch: 6 step: 124, loss is 9.951093\nepoch: 6 step: 125, loss is 6.370376\nepoch: 6 step: 126, loss is 11.832623\nepoch: 6 step: 127, loss is 9.702907\nepoch: 6 step: 128, loss is 6.6685543\nepoch time: 43824.170 ms, per step time: 342.376 ms\nepoch: 7 step: 1, loss is 6.502675\nepoch: 7 step: 2, loss is 9.168159\nepoch: 7 step: 3, loss is 4.958855\nepoch: 7 step: 4, loss is 6.1265745\nepoch: 7 step: 5, loss is 5.9276557\nepoch: 7 step: 6, loss is 6.456269\nepoch: 7 step: 7, loss is 5.9768863\nepoch: 7 step: 8, loss is 11.359106\nepoch: 7 step: 9, loss is 7.5607653\nepoch: 7 step: 10, loss is 9.245885\nepoch: 7 step: 11, loss is 7.2429085\nepoch: 7 step: 12, loss is 3.9333508\nepoch: 7 step: 13, loss is 2.8378532\nepoch: 7 step: 14, loss is 9.347883\nepoch: 7 step: 15, loss is 8.342447\nepoch: 7 step: 16, loss is 9.008419\nepoch: 7 step: 17, loss is 6.8598533\nepoch: 7 step: 18, loss is 6.959458\nepoch: 7 step: 19, loss is 6.594775\nepoch: 7 step: 20, loss is 8.154739\nepoch: 7 step: 21, loss is 11.119966\nepoch: 7 step: 22, loss is 12.097369\nepoch: 7 step: 23, loss is 6.574775\nepoch: 7 step: 24, loss is 3.7468202\nepoch: 7 step: 25, loss is 7.0470705\nepoch: 7 step: 26, loss is 12.136646\nepoch: 7 step: 27, loss is 10.7847395\nepoch: 7 step: 28, loss is 10.157968\nepoch: 7 step: 29, loss is 9.835083\nepoch: 7 step: 30, loss is 11.5223055\nepoch: 7 step: 31, loss is 5.8070345\nepoch: 7 step: 32, loss is 14.227971\nepoch: 7 step: 33, loss is 7.989815\nepoch: 7 step: 34, loss is 8.970303\nepoch: 7 step: 35, loss is 5.6371503\nepoch: 7 step: 36, loss is 2.0813448\nepoch: 7 step: 37, loss is 7.9063935\nepoch: 7 step: 38, loss is 12.05702\nepoch: 7 step: 39, loss is 11.248644\nepoch: 7 step: 40, loss is 12.058118\nepoch: 7 step: 41, loss is 7.3059306\nepoch: 7 step: 42, loss is 10.395231\nepoch: 7 step: 43, loss is 10.183519\nepoch: 7 step: 44, loss is 11.393917\nepoch: 7 step: 45, loss is 8.607256\nepoch: 7 step: 46, loss is 9.180984\nepoch: 7 step: 47, loss is 10.925264\nepoch: 7 step: 48, loss is 8.20084\nepoch: 7 step: 49, loss is 12.972393\nepoch: 7 step: 50, loss is 13.493985\nepoch: 7 step: 51, loss is 8.751359\nepoch: 7 step: 52, loss is 11.897821\nepoch: 7 step: 53, loss is 10.337568\nepoch: 7 step: 54, loss is 9.949832\nepoch: 7 step: 55, loss is 3.4667675\nepoch: 7 step: 56, loss is 9.0613785\nepoch: 7 step: 57, loss is 13.007854\nepoch: 7 step: 58, loss is 7.0536833\nepoch: 7 step: 59, loss is 9.072711\nepoch: 7 step: 60, loss is 8.975251\nepoch: 7 step: 61, loss is 8.842801\nepoch: 7 step: 62, loss is 8.578131\nepoch: 7 step: 63, loss is 6.5430284\nepoch: 7 step: 64, loss is 9.558474\nepoch: 7 step: 65, loss is 4.483513\nepoch: 7 step: 66, loss is 8.022745\nepoch: 7 step: 67, loss is 10.098683\nepoch: 7 step: 68, loss is 6.9172163\nepoch: 7 step: 69, loss is 7.867494\nepoch: 7 step: 70, loss is 12.655795\nepoch: 7 step: 71, loss is 8.478243\nepoch: 7 step: 72, loss is 6.8283033\nepoch: 7 step: 73, loss is 8.388402\nepoch: 7 step: 74, loss is 11.9273205\nepoch: 7 step: 75, loss is 8.815243\nepoch: 7 step: 76, loss is 4.882062\nepoch: 7 step: 77, loss is 16.273067\nepoch: 7 step: 78, loss is 8.306695\nepoch: 7 step: 79, loss is 10.416001\nepoch: 7 step: 80, loss is 6.843168\nepoch: 7 step: 81, loss is 6.112757\nepoch: 7 step: 82, loss is 9.613048\nepoch: 7 step: 83, loss is 8.952207\nepoch: 7 step: 84, loss is 10.005211\nepoch: 7 step: 85, loss is 8.633918\nepoch: 7 step: 86, loss is 12.060398\nepoch: 7 step: 87, loss is 10.968057\nepoch: 7 step: 88, loss is 17.235899\nepoch: 7 step: 89, loss is 5.141041\nepoch: 7 step: 90, loss is 10.399078\nepoch: 7 step: 91, loss is 10.6335125\nepoch: 7 step: 92, loss is 14.250596\nepoch: 7 step: 93, loss is 8.0174055\nepoch: 7 step: 94, loss is 8.993322\nepoch: 7 step: 95, loss is 8.945682\nepoch: 7 step: 96, loss is 13.954747\nepoch: 7 step: 97, loss is 12.133703\nepoch: 7 step: 98, loss is 9.937063\nepoch: 7 step: 99, loss is 16.941107\nepoch: 7 step: 100, loss is 7.527514\nepoch: 7 step: 101, loss is 8.2748785\nepoch: 7 step: 102, loss is 13.334481\nepoch: 7 step: 103, loss is 5.5595617\nepoch: 7 step: 104, loss is 7.029763\nepoch: 7 step: 105, loss is 6.730995\nepoch: 7 step: 106, loss is 10.0534\nepoch: 7 step: 107, loss is 8.922698\nepoch: 7 step: 108, loss is 8.053734\nepoch: 7 step: 109, loss is 14.346156\nepoch: 7 step: 110, loss is 5.9700828\nepoch: 7 step: 111, loss is 7.0827236\nepoch: 7 step: 112, loss is 4.311454\nepoch: 7 step: 113, loss is 7.784071\nepoch: 7 step: 114, loss is 7.9284916\nepoch: 7 step: 115, loss is 10.006755\nepoch: 7 step: 116, loss is 8.945493\nepoch: 7 step: 117, loss is 10.345797\nepoch: 7 step: 118, loss is 6.7082605\nepoch: 7 step: 119, loss is 12.761851\nepoch: 7 step: 120, loss is 9.628777\nepoch: 7 step: 121, loss is 7.4559264\nepoch: 7 step: 122, loss is 9.858399\nepoch: 7 step: 123, loss is 4.606998\nepoch: 7 step: 124, loss is 10.477966\nepoch: 7 step: 125, loss is 8.533861\nepoch: 7 step: 126, loss is 11.958599\nepoch: 7 step: 127, loss is 7.22126\nepoch: 7 step: 128, loss is 8.056628\nepoch time: 43772.420 ms, per step time: 341.972 ms\nepoch: 8 step: 1, loss is 4.024551\nepoch: 8 step: 2, loss is 3.6556878\nepoch: 8 step: 3, loss is 8.845316\nepoch: 8 step: 4, loss is 9.853269\nepoch: 8 step: 5, loss is 3.0157382\nepoch: 8 step: 6, loss is 10.10607\nepoch: 8 step: 7, loss is 7.033076\nepoch: 8 step: 8, loss is 5.12652\nepoch: 8 step: 9, loss is 8.843913\nepoch: 8 step: 10, loss is 6.7594028\nepoch: 8 step: 11, loss is 9.600083\nepoch: 8 step: 12, loss is 6.8682666\nepoch: 8 step: 13, loss is 7.664069\nepoch: 8 step: 14, loss is 7.0110316\nepoch: 8 step: 15, loss is 7.501819\nepoch: 8 step: 16, loss is 8.674952\nepoch: 8 step: 17, loss is 7.6288013\nepoch: 8 step: 18, loss is 7.509301\nepoch: 8 step: 19, loss is 6.7306547\nepoch: 8 step: 20, loss is 12.717833\nepoch: 8 step: 21, loss is 6.5827804\nepoch: 8 step: 22, loss is 8.430498\nepoch: 8 step: 23, loss is 12.483022\nepoch: 8 step: 24, loss is 12.211346\nepoch: 8 step: 25, loss is 7.575251\nepoch: 8 step: 26, loss is 13.440095\nepoch: 8 step: 27, loss is 5.8735747\nepoch: 8 step: 28, loss is 7.0366745\nepoch: 8 step: 29, loss is 9.057109\nepoch: 8 step: 30, loss is 12.912195\nepoch: 8 step: 31, loss is 10.966057\nepoch: 8 step: 32, loss is 10.829264\nepoch: 8 step: 33, loss is 8.550329\nepoch: 8 step: 34, loss is 8.813425\nepoch: 8 step: 35, loss is 6.196124\nepoch: 8 step: 36, loss is 10.8703575\nepoch: 8 step: 37, loss is 15.415176\nepoch: 8 step: 38, loss is 7.912864\nepoch: 8 step: 39, loss is 5.844098\nepoch: 8 step: 40, loss is 7.3743715\nepoch: 8 step: 41, loss is 13.457463\nepoch: 8 step: 42, loss is 11.983105\nepoch: 8 step: 43, loss is 7.029706\nepoch: 8 step: 44, loss is 6.698645\nepoch: 8 step: 45, loss is 7.257103\nepoch: 8 step: 46, loss is 10.845956\nepoch: 8 step: 47, loss is 8.173906\nepoch: 8 step: 48, loss is 7.23787\nepoch: 8 step: 49, loss is 15.105529\nepoch: 8 step: 50, loss is 4.799303\nepoch: 8 step: 51, loss is 8.68563\nepoch: 8 step: 52, loss is 9.274325\nepoch: 8 step: 53, loss is 9.011165\nepoch: 8 step: 54, loss is 10.360651\nepoch: 8 step: 55, loss is 4.8583875\nepoch: 8 step: 56, loss is 9.983913\nepoch: 8 step: 57, loss is 6.4801154\nepoch: 8 step: 58, loss is 6.2561183\nepoch: 8 step: 59, loss is 10.935246\nepoch: 8 step: 60, loss is 9.313889\nepoch: 8 step: 61, loss is 7.977309\nepoch: 8 step: 62, loss is 6.3178015\nepoch: 8 step: 63, loss is 4.728878\nepoch: 8 step: 64, loss is 11.768194\nepoch: 8 step: 65, loss is 7.9060254\nepoch: 8 step: 66, loss is 14.641476\nepoch: 8 step: 67, loss is 7.4943523\nepoch: 8 step: 68, loss is 4.883365\nepoch: 8 step: 69, loss is 6.8579135\nepoch: 8 step: 70, loss is 13.107025\nepoch: 8 step: 71, loss is 7.966153\nepoch: 8 step: 72, loss is 4.5640683\nepoch: 8 step: 73, loss is 6.562087\nepoch: 8 step: 74, loss is 6.349656\nepoch: 8 step: 75, loss is 9.808888\nepoch: 8 step: 76, loss is 8.528334\nepoch: 8 step: 77, loss is 15.42511\nepoch: 8 step: 78, loss is 9.565092\nepoch: 8 step: 79, loss is 10.618696\nepoch: 8 step: 80, loss is 10.038175\nepoch: 8 step: 81, loss is 10.062602\nepoch: 8 step: 82, loss is 9.043919\nepoch: 8 step: 83, loss is 10.700102\nepoch: 8 step: 84, loss is 6.0123997\nepoch: 8 step: 85, loss is 7.0128536\nepoch: 8 step: 86, loss is 6.931093\nepoch: 8 step: 87, loss is 9.997263\nepoch: 8 step: 88, loss is 6.4471974\nepoch: 8 step: 89, loss is 6.7735314\nepoch: 8 step: 90, loss is 11.742333\nepoch: 8 step: 91, loss is 7.5425267\nepoch: 8 step: 92, loss is 9.437133\nepoch: 8 step: 93, loss is 8.988313\nepoch: 8 step: 94, loss is 6.832842\nepoch: 8 step: 95, loss is 10.693013\nepoch: 8 step: 96, loss is 9.059848\nepoch: 8 step: 97, loss is 3.3594017\nepoch: 8 step: 98, loss is 11.968746\nepoch: 8 step: 99, loss is 9.795528\nepoch: 8 step: 100, loss is 9.232624\nepoch: 8 step: 101, loss is 11.408873\nepoch: 8 step: 102, loss is 11.914768\nepoch: 8 step: 103, loss is 7.0955286\nepoch: 8 step: 104, loss is 8.768481\nepoch: 8 step: 105, loss is 8.601799\nepoch: 8 step: 106, loss is 6.3031726\nepoch: 8 step: 107, loss is 6.421332\nepoch: 8 step: 108, loss is 16.890417\nepoch: 8 step: 109, loss is 12.024979\nepoch: 8 step: 110, loss is 12.965973\nepoch: 8 step: 111, loss is 8.588324\nepoch: 8 step: 112, loss is 10.821851\nepoch: 8 step: 113, loss is 6.8215632\nepoch: 8 step: 114, loss is 10.398924\nepoch: 8 step: 115, loss is 10.800125\nepoch: 8 step: 116, loss is 17.256714\nepoch: 8 step: 117, loss is 2.1731708\nepoch: 8 step: 118, loss is 12.596077\nepoch: 8 step: 119, loss is 9.481739\nepoch: 8 step: 120, loss is 11.767604\nepoch: 8 step: 121, loss is 9.333191\nepoch: 8 step: 122, loss is 5.025654\nepoch: 8 step: 123, loss is 12.960314\nepoch: 8 step: 124, loss is 6.5961394\nepoch: 8 step: 125, loss is 12.045084\nepoch: 8 step: 126, loss is 6.734912\nepoch: 8 step: 127, loss is 9.248161\nepoch: 8 step: 128, loss is 7.972059\nepoch time: 43807.474 ms, per step time: 342.246 ms\nepoch: 9 step: 1, loss is 9.827795\nepoch: 9 step: 2, loss is 5.3429484\nepoch: 9 step: 3, loss is 4.502539\nepoch: 9 step: 4, loss is 9.644525\nepoch: 9 step: 5, loss is 7.5004654\nepoch: 9 step: 6, loss is 4.496754\nepoch: 9 step: 7, loss is 5.081288\nepoch: 9 step: 8, loss is 8.081512\nepoch: 9 step: 9, loss is 8.5504465\nepoch: 9 step: 10, loss is 9.45105\nepoch: 9 step: 11, loss is 10.621729\nepoch: 9 step: 12, loss is 8.03874\nepoch: 9 step: 13, loss is 6.306827\nepoch: 9 step: 14, loss is 6.9664474\nepoch: 9 step: 15, loss is 5.641607\nepoch: 9 step: 16, loss is 6.154977\nepoch: 9 step: 17, loss is 15.911278\nepoch: 9 step: 18, loss is 5.5000052\nepoch: 9 step: 19, loss is 9.405564\nepoch: 9 step: 20, loss is 13.021885\nepoch: 9 step: 21, loss is 6.9692535\nepoch: 9 step: 22, loss is 6.2063847\nepoch: 9 step: 23, loss is 9.248534\nepoch: 9 step: 24, loss is 8.553354\nepoch: 9 step: 25, loss is 7.9296\nepoch: 9 step: 26, loss is 9.3219595\nepoch: 9 step: 27, loss is 3.9019642\nepoch: 9 step: 28, loss is 9.706495\nepoch: 9 step: 29, loss is 4.4284377\nepoch: 9 step: 30, loss is 6.997088\nepoch: 9 step: 31, loss is 10.773466\nepoch: 9 step: 32, loss is 6.2071633\nepoch: 9 step: 33, loss is 7.2102094\nepoch: 9 step: 34, loss is 11.483181\nepoch: 9 step: 35, loss is 8.483401\nepoch: 9 step: 36, loss is 12.613129\nepoch: 9 step: 37, loss is 11.037829\nepoch: 9 step: 38, loss is 10.028216\nepoch: 9 step: 39, loss is 10.295159\nepoch: 9 step: 40, loss is 14.157969\nepoch: 9 step: 41, loss is 12.126149\nepoch: 9 step: 42, loss is 9.014634\nepoch: 9 step: 43, loss is 5.483956\nepoch: 9 step: 44, loss is 11.933035\nepoch: 9 step: 45, loss is 8.043004\nepoch: 9 step: 46, loss is 10.511665\nepoch: 9 step: 47, loss is 9.3962755\nepoch: 9 step: 48, loss is 6.7488046\nepoch: 9 step: 49, loss is 16.28711\nepoch: 9 step: 50, loss is 4.5644326\nepoch: 9 step: 51, loss is 14.624428\nepoch: 9 step: 52, loss is 14.499769\nepoch: 9 step: 53, loss is 10.572676\nepoch: 9 step: 54, loss is 5.3993573\nepoch: 9 step: 55, loss is 14.360066\nepoch: 9 step: 56, loss is 10.307503\nepoch: 9 step: 57, loss is 8.659256\nepoch: 9 step: 58, loss is 8.472569\nepoch: 9 step: 59, loss is 4.728391\nepoch: 9 step: 60, loss is 11.844574\nepoch: 9 step: 61, loss is 7.7077703\nepoch: 9 step: 62, loss is 6.416305\nepoch: 9 step: 63, loss is 10.009106\nepoch: 9 step: 64, loss is 8.919536\nepoch: 9 step: 65, loss is 9.176658\nepoch: 9 step: 66, loss is 9.400658\nepoch: 9 step: 67, loss is 10.782668\nepoch: 9 step: 68, loss is 9.57072\nepoch: 9 step: 69, loss is 7.9017086\nepoch: 9 step: 70, loss is 12.630758\nepoch: 9 step: 71, loss is 8.6267395\nepoch: 9 step: 72, loss is 9.6104145\nepoch: 9 step: 73, loss is 5.261544\nepoch: 9 step: 74, loss is 8.030836\nepoch: 9 step: 75, loss is 5.978318\nepoch: 9 step: 76, loss is 7.6165037\nepoch: 9 step: 77, loss is 13.857055\nepoch: 9 step: 78, loss is 10.285347\nepoch: 9 step: 79, loss is 4.6030827\nepoch: 9 step: 80, loss is 10.30616\nepoch: 9 step: 81, loss is 6.7512455\nepoch: 9 step: 82, loss is 8.160736\nepoch: 9 step: 83, loss is 10.053701\nepoch: 9 step: 84, loss is 15.62794\nepoch: 9 step: 85, loss is 8.011565\nepoch: 9 step: 86, loss is 7.6849027\nepoch: 9 step: 87, loss is 2.770657\nepoch: 9 step: 88, loss is 8.305221\nepoch: 9 step: 89, loss is 6.082706\nepoch: 9 step: 90, loss is 7.43956\nepoch: 9 step: 91, loss is 13.617188\nepoch: 9 step: 92, loss is 9.913592\nepoch: 9 step: 93, loss is 10.655338\nepoch: 9 step: 94, loss is 9.719328\nepoch: 9 step: 95, loss is 8.764847\nepoch: 9 step: 96, loss is 6.2992883\nepoch: 9 step: 97, loss is 9.675755\nepoch: 9 step: 98, loss is 6.820412\nepoch: 9 step: 99, loss is 11.364159\nepoch: 9 step: 100, loss is 8.364976\nepoch: 9 step: 101, loss is 12.498596\nepoch: 9 step: 102, loss is 7.4728494\nepoch: 9 step: 103, loss is 16.011782\nepoch: 9 step: 104, loss is 8.980843\nepoch: 9 step: 105, loss is 12.207331\nepoch: 9 step: 106, loss is 13.723085\nepoch: 9 step: 107, loss is 7.8902717\nepoch: 9 step: 108, loss is 8.413345\nepoch: 9 step: 109, loss is 9.563429\nepoch: 9 step: 110, loss is 4.14932\nepoch: 9 step: 111, loss is 3.7532032\nepoch: 9 step: 112, loss is 7.119744\nepoch: 9 step: 113, loss is 9.971665\nepoch: 9 step: 114, loss is 9.341371\nepoch: 9 step: 115, loss is 8.846198\nepoch: 9 step: 116, loss is 5.319753\nepoch: 9 step: 117, loss is 10.800466\nepoch: 9 step: 118, loss is 15.37612\nepoch: 9 step: 119, loss is 15.921843\nepoch: 9 step: 120, loss is 10.445772\nepoch: 9 step: 121, loss is 15.710297\nepoch: 9 step: 122, loss is 13.286465\nepoch: 9 step: 123, loss is 10.671627\nepoch: 9 step: 124, loss is 10.962638\nepoch: 9 step: 125, loss is 11.536508\nepoch: 9 step: 126, loss is 9.935573\nepoch: 9 step: 127, loss is 9.892822\nepoch: 9 step: 128, loss is 13.352477\nepoch time: 43804.976 ms, per step time: 342.226 ms\nepoch: 10 step: 1, loss is 6.5099263\nepoch: 10 step: 2, loss is 5.0690894\nepoch: 10 step: 3, loss is 6.7650695\nepoch: 10 step: 4, loss is 8.452605\nepoch: 10 step: 5, loss is 9.300936\nepoch: 10 step: 6, loss is 9.551882\nepoch: 10 step: 7, loss is 7.477469\nepoch: 10 step: 8, loss is 9.160057\nepoch: 10 step: 9, loss is 5.677644\nepoch: 10 step: 10, loss is 10.891311\nepoch: 10 step: 11, loss is 5.8690915\nepoch: 10 step: 12, loss is 14.966089\nepoch: 10 step: 13, loss is 6.796867\nepoch: 10 step: 14, loss is 6.652707\nepoch: 10 step: 15, loss is 10.121213\nepoch: 10 step: 16, loss is 6.353669\nepoch: 10 step: 17, loss is 8.756094\nepoch: 10 step: 18, loss is 8.492231\nepoch: 10 step: 19, loss is 3.056807\nepoch: 10 step: 20, loss is 5.856588\nepoch: 10 step: 21, loss is 17.226137\nepoch: 10 step: 22, loss is 10.146313\nepoch: 10 step: 23, loss is 9.013048\nepoch: 10 step: 24, loss is 9.647504\nepoch: 10 step: 25, loss is 11.327115\nepoch: 10 step: 26, loss is 11.849559\nepoch: 10 step: 27, loss is 7.501987\nepoch: 10 step: 28, loss is 9.975269\nepoch: 10 step: 29, loss is 7.609107\nepoch: 10 step: 30, loss is 11.425719\nepoch: 10 step: 31, loss is 6.119761\nepoch: 10 step: 32, loss is 11.862829\nepoch: 10 step: 33, loss is 14.017698\nepoch: 10 step: 34, loss is 14.634377\nepoch: 10 step: 35, loss is 6.2210975\nepoch: 10 step: 36, loss is 7.0027676\nepoch: 10 step: 37, loss is 13.285527\nepoch: 10 step: 38, loss is 6.278391\nepoch: 10 step: 39, loss is 7.9734464\nepoch: 10 step: 40, loss is 9.042862\nepoch: 10 step: 41, loss is 9.613925\nepoch: 10 step: 42, loss is 6.7120757\nepoch: 10 step: 43, loss is 13.181273\nepoch: 10 step: 44, loss is 4.367959\nepoch: 10 step: 45, loss is 8.641939\nepoch: 10 step: 46, loss is 12.906349\nepoch: 10 step: 47, loss is 6.083489\nepoch: 10 step: 48, loss is 9.344283\nepoch: 10 step: 49, loss is 11.958947\nepoch: 10 step: 50, loss is 4.8460426\nepoch: 10 step: 51, loss is 3.245501\nepoch: 10 step: 52, loss is 6.3804092\nepoch: 10 step: 53, loss is 7.7399173\nepoch: 10 step: 54, loss is 7.332916\nepoch: 10 step: 55, loss is 9.284428\nepoch: 10 step: 56, loss is 8.940633\nepoch: 10 step: 57, loss is 6.466911\nepoch: 10 step: 58, loss is 10.329097\nepoch: 10 step: 59, loss is 7.1959586\nepoch: 10 step: 60, loss is 6.8244214\nepoch: 10 step: 61, loss is 7.239441\nepoch: 10 step: 62, loss is 8.698698\nepoch: 10 step: 63, loss is 8.687141\nepoch: 10 step: 64, loss is 7.4071374\nepoch: 10 step: 65, loss is 7.0234046\nepoch: 10 step: 66, loss is 11.900303\nepoch: 10 step: 67, loss is 8.405843\nepoch: 10 step: 68, loss is 5.671785\nepoch: 10 step: 69, loss is 14.660776\nepoch: 10 step: 70, loss is 4.149484\nepoch: 10 step: 71, loss is 7.063553\nepoch: 10 step: 72, loss is 6.380935\nepoch: 10 step: 73, loss is 13.00575\nepoch: 10 step: 74, loss is 10.562679\nepoch: 10 step: 75, loss is 9.051017\nepoch: 10 step: 76, loss is 8.61026\nepoch: 10 step: 77, loss is 8.825232\nepoch: 10 step: 78, loss is 7.5764465\nepoch: 10 step: 79, loss is 6.843147\nepoch: 10 step: 80, loss is 5.1214304\nepoch: 10 step: 81, loss is 6.241983\nepoch: 10 step: 82, loss is 5.0073924\nepoch: 10 step: 83, loss is 9.713245\nepoch: 10 step: 84, loss is 6.576173\nepoch: 10 step: 85, loss is 5.229015\nepoch: 10 step: 86, loss is 6.56054\nepoch: 10 step: 87, loss is 10.487764\nepoch: 10 step: 88, loss is 10.374606\nepoch: 10 step: 89, loss is 12.094724\nepoch: 10 step: 90, loss is 7.6748595\nepoch: 10 step: 91, loss is 6.494276\nepoch: 10 step: 92, loss is 8.682762\nepoch: 10 step: 93, loss is 6.246069\nepoch: 10 step: 94, loss is 9.220118\nepoch: 10 step: 95, loss is 5.491951\nepoch: 10 step: 96, loss is 10.373545\nepoch: 10 step: 97, loss is 5.1418123\nepoch: 10 step: 98, loss is 14.939544\nepoch: 10 step: 99, loss is 6.6143413\nepoch: 10 step: 100, loss is 6.7405\nepoch: 10 step: 101, loss is 8.294314\nepoch: 10 step: 102, loss is 10.425117\nepoch: 10 step: 103, loss is 9.659581\nepoch: 10 step: 104, loss is 11.472656\nepoch: 10 step: 105, loss is 11.35461\nepoch: 10 step: 106, loss is 9.658384\nepoch: 10 step: 107, loss is 9.757344\nepoch: 10 step: 108, loss is 12.77427\nepoch: 10 step: 109, loss is 7.098624\nepoch: 10 step: 110, loss is 10.115437\nepoch: 10 step: 111, loss is 7.7526593\nepoch: 10 step: 112, loss is 14.992369\nepoch: 10 step: 113, loss is 13.450154\nepoch: 10 step: 114, loss is 10.429766\nepoch: 10 step: 115, loss is 12.269516\nepoch: 10 step: 116, loss is 8.325698\nepoch: 10 step: 117, loss is 9.349454\nepoch: 10 step: 118, loss is 5.5414124\nepoch: 10 step: 119, loss is 9.815628\nepoch: 10 step: 120, loss is 18.601437\nepoch: 10 step: 121, loss is 9.15806\nepoch: 10 step: 122, loss is 11.630963\nepoch: 10 step: 123, loss is 10.224414\nepoch: 10 step: 124, loss is 9.314666\nepoch: 10 step: 125, loss is 8.441544\nepoch: 10 step: 126, loss is 11.9455185\nepoch: 10 step: 127, loss is 4.1888967\nepoch: 10 step: 128, loss is 6.183711\nepoch time: 43816.869 ms, per step time: 342.319 ms\nepoch: 11 step: 1, loss is 3.984032\nepoch: 11 step: 2, loss is 6.1061873\nepoch: 11 step: 3, loss is 13.123467\nepoch: 11 step: 4, loss is 4.7215705\nepoch: 11 step: 5, loss is 7.1014075\nepoch: 11 step: 6, loss is 6.820935\nepoch: 11 step: 7, loss is 12.472719\nepoch: 11 step: 8, loss is 9.114802\nepoch: 11 step: 9, loss is 15.384155\nepoch: 11 step: 10, loss is 7.9009404\nepoch: 11 step: 11, loss is 12.381216\nepoch: 11 step: 12, loss is 6.228404\nepoch: 11 step: 13, loss is 8.697912\nepoch: 11 step: 14, loss is 9.114064\nepoch: 11 step: 15, loss is 10.142875\nepoch: 11 step: 16, loss is 7.0430627\nepoch: 11 step: 17, loss is 13.08285\nepoch: 11 step: 18, loss is 6.222992\nepoch: 11 step: 19, loss is 13.307453\nepoch: 11 step: 20, loss is 8.47585\nepoch: 11 step: 21, loss is 14.981814\nepoch: 11 step: 22, loss is 6.647124\nepoch: 11 step: 23, loss is 9.994829\nepoch: 11 step: 24, loss is 6.4929442\nepoch: 11 step: 25, loss is 8.76218\nepoch: 11 step: 26, loss is 4.786301\nepoch: 11 step: 27, loss is 8.042532\nepoch: 11 step: 28, loss is 3.4344447\nepoch: 11 step: 29, loss is 4.8695035\nepoch: 11 step: 30, loss is 9.638462\nepoch: 11 step: 31, loss is 11.069671\nepoch: 11 step: 32, loss is 9.311236\nepoch: 11 step: 33, loss is 15.149097\nepoch: 11 step: 34, loss is 6.1132865\nepoch: 11 step: 35, loss is 10.559633\nepoch: 11 step: 36, loss is 7.3485136\nepoch: 11 step: 37, loss is 8.793553\nepoch: 11 step: 38, loss is 9.053821\nepoch: 11 step: 39, loss is 7.3967476\nepoch: 11 step: 40, loss is 3.5426598\nepoch: 11 step: 41, loss is 5.8699465\nepoch: 11 step: 42, loss is 12.210892\nepoch: 11 step: 43, loss is 13.110189\nepoch: 11 step: 44, loss is 7.155567\nepoch: 11 step: 45, loss is 11.54961\nepoch: 11 step: 46, loss is 9.544518\nepoch: 11 step: 47, loss is 8.758809\nepoch: 11 step: 48, loss is 9.194851\nepoch: 11 step: 49, loss is 9.548172\nepoch: 11 step: 50, loss is 5.3620815\nepoch: 11 step: 51, loss is 14.729812\nepoch: 11 step: 52, loss is 4.775372\nepoch: 11 step: 53, loss is 12.883384\nepoch: 11 step: 54, loss is 5.526913\nepoch: 11 step: 55, loss is 9.494065\nepoch: 11 step: 56, loss is 7.444721\nepoch: 11 step: 57, loss is 10.568393\nepoch: 11 step: 58, loss is 15.740184\nepoch: 11 step: 59, loss is 13.048641\nepoch: 11 step: 60, loss is 7.3850408\nepoch: 11 step: 61, loss is 10.0971365\nepoch: 11 step: 62, loss is 10.506306\nepoch: 11 step: 63, loss is 5.943881\nepoch: 11 step: 64, loss is 17.724277\nepoch: 11 step: 65, loss is 7.550124\nepoch: 11 step: 66, loss is 6.88191\nepoch: 11 step: 67, loss is 5.0382833\nepoch: 11 step: 68, loss is 6.9353013\nepoch: 11 step: 69, loss is 11.594261\nepoch: 11 step: 70, loss is 13.251365\nepoch: 11 step: 71, loss is 4.56907\nepoch: 11 step: 72, loss is 5.712782\nepoch: 11 step: 73, loss is 8.619141\nepoch: 11 step: 74, loss is 7.719324\nepoch: 11 step: 75, loss is 13.819293\nepoch: 11 step: 76, loss is 7.2258677\nepoch: 11 step: 77, loss is 4.3898554\nepoch: 11 step: 78, loss is 6.170697\nepoch: 11 step: 79, loss is 5.0316815\nepoch: 11 step: 80, loss is 9.928365\nepoch: 11 step: 81, loss is 9.933171\nepoch: 11 step: 82, loss is 4.2836504\nepoch: 11 step: 83, loss is 9.058024\nepoch: 11 step: 84, loss is 11.747563\nepoch: 11 step: 85, loss is 8.011991\nepoch: 11 step: 86, loss is 9.873371\nepoch: 11 step: 87, loss is 6.071984\nepoch: 11 step: 88, loss is 16.756413\nepoch: 11 step: 89, loss is 5.545394\nepoch: 11 step: 90, loss is 17.591366\nepoch: 11 step: 91, loss is 8.865626\nepoch: 11 step: 92, loss is 6.3293333\nepoch: 11 step: 93, loss is 15.006778\nepoch: 11 step: 94, loss is 12.083362\nepoch: 11 step: 95, loss is 14.16432\nepoch: 11 step: 96, loss is 13.463557\nepoch: 11 step: 97, loss is 5.7902923\nepoch: 11 step: 98, loss is 7.636753\nepoch: 11 step: 99, loss is 16.748701\nepoch: 11 step: 100, loss is 14.383204\nepoch: 11 step: 101, loss is 8.897147\nepoch: 11 step: 102, loss is 9.157109\nepoch: 11 step: 103, loss is 14.94152\nepoch: 11 step: 104, loss is 9.162371\nepoch: 11 step: 105, loss is 6.9062357\nepoch: 11 step: 106, loss is 10.096049\nepoch: 11 step: 107, loss is 9.060239\nepoch: 11 step: 108, loss is 6.544908\nepoch: 11 step: 109, loss is 5.525392\nepoch: 11 step: 110, loss is 16.21973\nepoch: 11 step: 111, loss is 5.870253\nepoch: 11 step: 112, loss is 6.370179\nepoch: 11 step: 113, loss is 9.605986\nepoch: 11 step: 114, loss is 8.610231\nepoch: 11 step: 115, loss is 10.742289\nepoch: 11 step: 116, loss is 9.499393\nepoch: 11 step: 117, loss is 11.780115\nepoch: 11 step: 118, loss is 9.014155\nepoch: 11 step: 119, loss is 6.304848\nepoch: 11 step: 120, loss is 8.865583\nepoch: 11 step: 121, loss is 5.0495996\nepoch: 11 step: 122, loss is 7.2738853\nepoch: 11 step: 123, loss is 6.5264564\nepoch: 11 step: 124, loss is 6.1796374\nepoch: 11 step: 125, loss is 5.702194\nepoch: 11 step: 126, loss is 9.946026\nepoch: 11 step: 127, loss is 13.435892\nepoch: 11 step: 128, loss is 2.5804768\nepoch time: 43776.846 ms, per step time: 342.007 ms\nepoch: 12 step: 1, loss is 7.5844474\nepoch: 12 step: 2, loss is 4.960154\nepoch: 12 step: 3, loss is 4.268857\nepoch: 12 step: 4, loss is 4.6312723\nepoch: 12 step: 5, loss is 4.8346353\nepoch: 12 step: 6, loss is 3.6539059\nepoch: 12 step: 7, loss is 12.200442\nepoch: 12 step: 8, loss is 13.5975\nepoch: 12 step: 9, loss is 11.527701\nepoch: 12 step: 10, loss is 6.669078\nepoch: 12 step: 11, loss is 3.640597\nepoch: 12 step: 12, loss is 8.473224\nepoch: 12 step: 13, loss is 5.493008\nepoch: 12 step: 14, loss is 17.604364\nepoch: 12 step: 15, loss is 9.358901\nepoch: 12 step: 16, loss is 8.849909\nepoch: 12 step: 17, loss is 8.096769\nepoch: 12 step: 18, loss is 8.607992\nepoch: 12 step: 19, loss is 8.603823\nepoch: 12 step: 20, loss is 3.3932567\nepoch: 12 step: 21, loss is 7.0207\nepoch: 12 step: 22, loss is 8.754209\nepoch: 12 step: 23, loss is 8.150162\nepoch: 12 step: 24, loss is 8.811006\nepoch: 12 step: 25, loss is 7.781875\nepoch: 12 step: 26, loss is 9.559587\nepoch: 12 step: 27, loss is 11.226373\nepoch: 12 step: 28, loss is 3.5779848\nepoch: 12 step: 29, loss is 5.613099\nepoch: 12 step: 30, loss is 4.9396925\nepoch: 12 step: 31, loss is 7.435673\nepoch: 12 step: 32, loss is 4.7684083\nepoch: 12 step: 33, loss is 9.826801\nepoch: 12 step: 34, loss is 12.547859\nepoch: 12 step: 35, loss is 5.2665405\nepoch: 12 step: 36, loss is 8.531301\nepoch: 12 step: 37, loss is 7.2615747\nepoch: 12 step: 38, loss is 7.112138\nepoch: 12 step: 39, loss is 3.8731585\nepoch: 12 step: 40, loss is 4.9428587\nepoch: 12 step: 41, loss is 12.454203\nepoch: 12 step: 42, loss is 7.7270856\nepoch: 12 step: 43, loss is 9.550043\nepoch: 12 step: 44, loss is 5.248613\nepoch: 12 step: 45, loss is 3.7900496\nepoch: 12 step: 46, loss is 6.5664253\nepoch: 12 step: 47, loss is 7.9672976\nepoch: 12 step: 48, loss is 9.64096\nepoch: 12 step: 49, loss is 6.5545206\nepoch: 12 step: 50, loss is 8.6948\nepoch: 12 step: 51, loss is 6.3125362\nepoch: 12 step: 52, loss is 6.819556\nepoch: 12 step: 53, loss is 3.9803753\nepoch: 12 step: 54, loss is 6.329228\nepoch: 12 step: 55, loss is 6.3752823\nepoch: 12 step: 56, loss is 9.175474\nepoch: 12 step: 57, loss is 10.76072\nepoch: 12 step: 58, loss is 7.2064643\nepoch: 12 step: 59, loss is 10.459954\nepoch: 12 step: 60, loss is 12.82186\nepoch: 12 step: 61, loss is 10.671102\nepoch: 12 step: 62, loss is 2.9893007\nepoch: 12 step: 63, loss is 5.875527\nepoch: 12 step: 64, loss is 10.253565\nepoch: 12 step: 65, loss is 6.381545\nepoch: 12 step: 66, loss is 8.366463\nepoch: 12 step: 67, loss is 9.789709\nepoch: 12 step: 68, loss is 13.473035\nepoch: 12 step: 69, loss is 7.7029386\nepoch: 12 step: 70, loss is 7.5702233\nepoch: 12 step: 71, loss is 8.958621\nepoch: 12 step: 72, loss is 13.435019\nepoch: 12 step: 73, loss is 7.1407466\nepoch: 12 step: 74, loss is 10.943805\nepoch: 12 step: 75, loss is 11.087059\nepoch: 12 step: 76, loss is 12.259202\nepoch: 12 step: 77, loss is 12.332201\nepoch: 12 step: 78, loss is 11.6905155\nepoch: 12 step: 79, loss is 10.052266\nepoch: 12 step: 80, loss is 13.838628\nepoch: 12 step: 81, loss is 6.004493\nepoch: 12 step: 82, loss is 3.7871723\nepoch: 12 step: 83, loss is 10.843331\nepoch: 12 step: 84, loss is 7.143737\nepoch: 12 step: 85, loss is 12.025925\nepoch: 12 step: 86, loss is 10.540718\nepoch: 12 step: 87, loss is 4.329341\nepoch: 12 step: 88, loss is 11.935604\nepoch: 12 step: 89, loss is 8.383731\nepoch: 12 step: 90, loss is 11.164094\nepoch: 12 step: 91, loss is 14.709889\nepoch: 12 step: 92, loss is 12.125233\nepoch: 12 step: 93, loss is 12.25129\nepoch: 12 step: 94, loss is 11.123477\nepoch: 12 step: 95, loss is 9.041665\nepoch: 12 step: 96, loss is 8.301527\nepoch: 12 step: 97, loss is 8.103565\nepoch: 12 step: 98, loss is 8.21025\nepoch: 12 step: 99, loss is 9.697388\nepoch: 12 step: 100, loss is 4.0785193\nepoch: 12 step: 101, loss is 10.052394\nepoch: 12 step: 102, loss is 6.3173018\nepoch: 12 step: 103, loss is 7.7851357\nepoch: 12 step: 104, loss is 9.145553\nepoch: 12 step: 105, loss is 13.802358\nepoch: 12 step: 106, loss is 4.580615\nepoch: 12 step: 107, loss is 9.664547\nepoch: 12 step: 108, loss is 12.542956\nepoch: 12 step: 109, loss is 7.942417\nepoch: 12 step: 110, loss is 15.126367\nepoch: 12 step: 111, loss is 4.040099\nepoch: 12 step: 112, loss is 4.8953233\nepoch: 12 step: 113, loss is 9.74248\nepoch: 12 step: 114, loss is 12.981571\nepoch: 12 step: 115, loss is 10.128571\nepoch: 12 step: 116, loss is 11.010602\nepoch: 12 step: 117, loss is 7.84328\nepoch: 12 step: 118, loss is 13.783063\nepoch: 12 step: 119, loss is 12.013272\nepoch: 12 step: 120, loss is 3.0889013\nepoch: 12 step: 121, loss is 5.143147\nepoch: 12 step: 122, loss is 6.722185\nepoch: 12 step: 123, loss is 8.1271\nepoch: 12 step: 124, loss is 9.231514\nepoch: 12 step: 125, loss is 12.13796\nepoch: 12 step: 126, loss is 6.5351872\nepoch: 12 step: 127, loss is 11.261668\nepoch: 12 step: 128, loss is 6.5311065\nepoch time: 43797.781 ms, per step time: 342.170 ms\nepoch: 13 step: 1, loss is 5.1376715\nepoch: 13 step: 2, loss is 10.09428\nepoch: 13 step: 3, loss is 7.224064\nepoch: 13 step: 4, loss is 6.972173\nepoch: 13 step: 5, loss is 6.261872\nepoch: 13 step: 6, loss is 4.9706163\nepoch: 13 step: 7, loss is 5.198844\nepoch: 13 step: 8, loss is 4.4769616\nepoch: 13 step: 9, loss is 5.4602\nepoch: 13 step: 10, loss is 5.047699\nepoch: 13 step: 11, loss is 7.2817335\nepoch: 13 step: 12, loss is 11.02119\nepoch: 13 step: 13, loss is 6.309455\nepoch: 13 step: 14, loss is 6.7204785\nepoch: 13 step: 15, loss is 5.2415648\nepoch: 13 step: 16, loss is 8.566316\nepoch: 13 step: 17, loss is 6.541852\nepoch: 13 step: 18, loss is 6.8132625\nepoch: 13 step: 19, loss is 11.183663\nepoch: 13 step: 20, loss is 6.695988\nepoch: 13 step: 21, loss is 9.163305\nepoch: 13 step: 22, loss is 7.0427814\nepoch: 13 step: 23, loss is 11.895219\nepoch: 13 step: 24, loss is 13.756317\nepoch: 13 step: 25, loss is 11.884094\nepoch: 13 step: 26, loss is 8.41325\nepoch: 13 step: 27, loss is 8.806627\nepoch: 13 step: 28, loss is 11.709672\nepoch: 13 step: 29, loss is 16.201862\nepoch: 13 step: 30, loss is 7.273795\nepoch: 13 step: 31, loss is 14.000238\nepoch: 13 step: 32, loss is 10.521747\nepoch: 13 step: 33, loss is 8.218131\nepoch: 13 step: 34, loss is 10.61129\nepoch: 13 step: 35, loss is 7.0666695\nepoch: 13 step: 36, loss is 7.952732\nepoch: 13 step: 37, loss is 7.6690483\nepoch: 13 step: 38, loss is 10.637189\nepoch: 13 step: 39, loss is 5.9742527\nepoch: 13 step: 40, loss is 6.0326176\nepoch: 13 step: 41, loss is 11.954278\nepoch: 13 step: 42, loss is 5.5319457\nepoch: 13 step: 43, loss is 9.217514\nepoch: 13 step: 44, loss is 11.193379\nepoch: 13 step: 45, loss is 14.215434\nepoch: 13 step: 46, loss is 8.095669\nepoch: 13 step: 47, loss is 9.174644\nepoch: 13 step: 48, loss is 11.377034\nepoch: 13 step: 49, loss is 9.110058\nepoch: 13 step: 50, loss is 14.77843\nepoch: 13 step: 51, loss is 12.470275\nepoch: 13 step: 52, loss is 12.992997\nepoch: 13 step: 53, loss is 8.708241\nepoch: 13 step: 54, loss is 10.420681\nepoch: 13 step: 55, loss is 14.481148\nepoch: 13 step: 56, loss is 7.955453\nepoch: 13 step: 57, loss is 9.175232\nepoch: 13 step: 58, loss is 7.0105495\nepoch: 13 step: 59, loss is 10.975578\nepoch: 13 step: 60, loss is 10.132048\nepoch: 13 step: 61, loss is 7.1218467\nepoch: 13 step: 62, loss is 10.6243515\nepoch: 13 step: 63, loss is 11.682983\nepoch: 13 step: 64, loss is 6.5630264\nepoch: 13 step: 65, loss is 5.0048165\nepoch: 13 step: 66, loss is 7.771958\nepoch: 13 step: 67, loss is 3.897884\nepoch: 13 step: 68, loss is 10.720207\nepoch: 13 step: 69, loss is 6.66596\nepoch: 13 step: 70, loss is 10.561336\nepoch: 13 step: 71, loss is 6.0396533\nepoch: 13 step: 72, loss is 10.695895\nepoch: 13 step: 73, loss is 10.993376\nepoch: 13 step: 74, loss is 10.866937\nepoch: 13 step: 75, loss is 15.41766\nepoch: 13 step: 76, loss is 8.584387\nepoch: 13 step: 77, loss is 8.183601\nepoch: 13 step: 78, loss is 9.703675\nepoch: 13 step: 79, loss is 9.926603\nepoch: 13 step: 80, loss is 6.5635285\nepoch: 13 step: 81, loss is 15.9351\nepoch: 13 step: 82, loss is 7.323168\nepoch: 13 step: 83, loss is 13.037364\nepoch: 13 step: 84, loss is 6.7791934\nepoch: 13 step: 85, loss is 12.743265\nepoch: 13 step: 86, loss is 10.108184\nepoch: 13 step: 87, loss is 16.623053\nepoch: 13 step: 88, loss is 9.135036\nepoch: 13 step: 89, loss is 8.057396\nepoch: 13 step: 90, loss is 10.667106\nepoch: 13 step: 91, loss is 8.418913\nepoch: 13 step: 92, loss is 13.327117\nepoch: 13 step: 93, loss is 5.910031\nepoch: 13 step: 94, loss is 11.51511\nepoch: 13 step: 95, loss is 15.776101\nepoch: 13 step: 96, loss is 10.074626\nepoch: 13 step: 97, loss is 9.741626\nepoch: 13 step: 98, loss is 7.498474\nepoch: 13 step: 99, loss is 12.812368\nepoch: 13 step: 100, loss is 4.430304\nepoch: 13 step: 101, loss is 15.039809\nepoch: 13 step: 102, loss is 4.430572\nepoch: 13 step: 103, loss is 6.7187905\nepoch: 13 step: 104, loss is 5.1069922\nepoch: 13 step: 105, loss is 10.243032\nepoch: 13 step: 106, loss is 12.112712\nepoch: 13 step: 107, loss is 7.896402\nepoch: 13 step: 108, loss is 14.292752\nepoch: 13 step: 109, loss is 13.200877\nepoch: 13 step: 110, loss is 7.3923125\nepoch: 13 step: 111, loss is 9.788137\nepoch: 13 step: 112, loss is 8.071705\nepoch: 13 step: 113, loss is 8.775586\nepoch: 13 step: 114, loss is 3.3313632\nepoch: 13 step: 115, loss is 7.4430428\nepoch: 13 step: 116, loss is 17.045494\nepoch: 13 step: 117, loss is 7.198073\nepoch: 13 step: 118, loss is 9.362522\nepoch: 13 step: 119, loss is 11.136161\nepoch: 13 step: 120, loss is 7.3888497\nepoch: 13 step: 121, loss is 6.8079\nepoch: 13 step: 122, loss is 8.650767\nepoch: 13 step: 123, loss is 9.631949\nepoch: 13 step: 124, loss is 20.66797\nepoch: 13 step: 125, loss is 12.235354\nepoch: 13 step: 126, loss is 11.343374\nepoch: 13 step: 127, loss is 6.6506333\nepoch: 13 step: 128, loss is 12.222833\nepoch time: 43949.147 ms, per step time: 343.353 ms\nepoch: 14 step: 1, loss is 10.108598\nepoch: 14 step: 2, loss is 6.165353\nepoch: 14 step: 3, loss is 6.3137283\nepoch: 14 step: 4, loss is 7.3544383\nepoch: 14 step: 5, loss is 8.197413\nepoch: 14 step: 6, loss is 11.695793\nepoch: 14 step: 7, loss is 5.6336174\nepoch: 14 step: 8, loss is 8.435317\nepoch: 14 step: 9, loss is 6.464847\nepoch: 14 step: 10, loss is 7.7187004\nepoch: 14 step: 11, loss is 4.8189087\nepoch: 14 step: 12, loss is 7.6313105\nepoch: 14 step: 13, loss is 8.630284\nepoch: 14 step: 14, loss is 8.212988\nepoch: 14 step: 15, loss is 4.610127\nepoch: 14 step: 16, loss is 11.330706\nepoch: 14 step: 17, loss is 8.593532\nepoch: 14 step: 18, loss is 7.2731795\nepoch: 14 step: 19, loss is 7.7911196\nepoch: 14 step: 20, loss is 10.630618\nepoch: 14 step: 21, loss is 9.88707\nepoch: 14 step: 22, loss is 5.0707784\nepoch: 14 step: 23, loss is 11.466897\nepoch: 14 step: 24, loss is 12.053906\nepoch: 14 step: 25, loss is 6.3674693\nepoch: 14 step: 26, loss is 7.3270946\nepoch: 14 step: 27, loss is 11.5117855\nepoch: 14 step: 28, loss is 10.344539\nepoch: 14 step: 29, loss is 9.225415\nepoch: 14 step: 30, loss is 6.961207\nepoch: 14 step: 31, loss is 11.631351\nepoch: 14 step: 32, loss is 6.219175\nepoch: 14 step: 33, loss is 11.273455\nepoch: 14 step: 34, loss is 12.285479\nepoch: 14 step: 35, loss is 10.534099\nepoch: 14 step: 36, loss is 10.416897\nepoch: 14 step: 37, loss is 3.7436795\nepoch: 14 step: 38, loss is 8.049452\nepoch: 14 step: 39, loss is 13.9244995\nepoch: 14 step: 40, loss is 10.432158\nepoch: 14 step: 41, loss is 12.9717245\nepoch: 14 step: 42, loss is 10.325692\nepoch: 14 step: 43, loss is 6.715148\nepoch: 14 step: 44, loss is 8.272806\nepoch: 14 step: 45, loss is 10.480613\nepoch: 14 step: 46, loss is 4.9005003\nepoch: 14 step: 47, loss is 5.9089427\nepoch: 14 step: 48, loss is 9.82815\nepoch: 14 step: 49, loss is 5.9976816\nepoch: 14 step: 50, loss is 8.081743\nepoch: 14 step: 51, loss is 11.309001\nepoch: 14 step: 52, loss is 8.988569\nepoch: 14 step: 53, loss is 6.481787\nepoch: 14 step: 54, loss is 9.960695\nepoch: 14 step: 55, loss is 10.960129\nepoch: 14 step: 56, loss is 8.695699\nepoch: 14 step: 57, loss is 9.4996805\nepoch: 14 step: 58, loss is 10.051893\nepoch: 14 step: 59, loss is 11.522561\nepoch: 14 step: 60, loss is 11.335181\nepoch: 14 step: 61, loss is 9.188865\nepoch: 14 step: 62, loss is 9.375468\nepoch: 14 step: 63, loss is 7.6346817\nepoch: 14 step: 64, loss is 16.098015\nepoch: 14 step: 65, loss is 7.935781\nepoch: 14 step: 66, loss is 8.523249\nepoch: 14 step: 67, loss is 7.698739\nepoch: 14 step: 68, loss is 4.3368173\nepoch: 14 step: 69, loss is 6.276509\nepoch: 14 step: 70, loss is 11.628762\nepoch: 14 step: 71, loss is 4.0402\nepoch: 14 step: 72, loss is 5.335348\nepoch: 14 step: 73, loss is 19.041023\nepoch: 14 step: 74, loss is 7.037915\nepoch: 14 step: 75, loss is 13.910503\nepoch: 14 step: 76, loss is 8.619887\nepoch: 14 step: 77, loss is 7.8648324\nepoch: 14 step: 78, loss is 9.463927\nepoch: 14 step: 79, loss is 8.991876\nepoch: 14 step: 80, loss is 9.494975\nepoch: 14 step: 81, loss is 7.085328\nepoch: 14 step: 82, loss is 5.762867\nepoch: 14 step: 83, loss is 10.73827\nepoch: 14 step: 84, loss is 11.172317\nepoch: 14 step: 85, loss is 23.493538\nepoch: 14 step: 86, loss is 5.1317925\nepoch: 14 step: 87, loss is 11.679813\nepoch: 14 step: 88, loss is 7.043165\nepoch: 14 step: 89, loss is 10.618332\nepoch: 14 step: 90, loss is 8.81175\nepoch: 14 step: 91, loss is 9.382765\nepoch: 14 step: 92, loss is 17.889519\nepoch: 14 step: 93, loss is 11.331121\nepoch: 14 step: 94, loss is 12.490784\nepoch: 14 step: 95, loss is 8.116252\nepoch: 14 step: 96, loss is 7.941783\nepoch: 14 step: 97, loss is 7.021739\nepoch: 14 step: 98, loss is 7.9222994\nepoch: 14 step: 99, loss is 8.494708\nepoch: 14 step: 100, loss is 13.588296\nepoch: 14 step: 101, loss is 11.79457\nepoch: 14 step: 102, loss is 8.87087\nepoch: 14 step: 103, loss is 9.915356\nepoch: 14 step: 104, loss is 11.019046\nepoch: 14 step: 105, loss is 9.319281\nepoch: 14 step: 106, loss is 16.412273\nepoch: 14 step: 107, loss is 10.387045\nepoch: 14 step: 108, loss is 6.963062\nepoch: 14 step: 109, loss is 9.286154\nepoch: 14 step: 110, loss is 3.5198402\nepoch: 14 step: 111, loss is 5.14196\nepoch: 14 step: 112, loss is 9.312618\nepoch: 14 step: 113, loss is 8.624567\nepoch: 14 step: 114, loss is 22.390589\nepoch: 14 step: 115, loss is 7.8292813\nepoch: 14 step: 116, loss is 14.3791065\nepoch: 14 step: 117, loss is 14.381289\nepoch: 14 step: 118, loss is 17.399605\nepoch: 14 step: 119, loss is 11.360186\nepoch: 14 step: 120, loss is 11.827004\nepoch: 14 step: 121, loss is 8.243978\nepoch: 14 step: 122, loss is 11.319073\nepoch: 14 step: 123, loss is 8.32876\nepoch: 14 step: 124, loss is 7.0095\nepoch: 14 step: 125, loss is 10.099317\nepoch: 14 step: 126, loss is 9.13187\nepoch: 14 step: 127, loss is 3.726005\nepoch: 14 step: 128, loss is 15.771011\nepoch time: 43874.707 ms, per step time: 342.771 ms\nepoch: 15 step: 1, loss is 4.97406\nepoch: 15 step: 2, loss is 3.7078207\nepoch: 15 step: 3, loss is 9.469034\nepoch: 15 step: 4, loss is 11.338221\nepoch: 15 step: 5, loss is 3.6260517\nepoch: 15 step: 6, loss is 9.595332\nepoch: 15 step: 7, loss is 4.2506094\nepoch: 15 step: 8, loss is 8.372445\nepoch: 15 step: 9, loss is 9.052384\nepoch: 15 step: 10, loss is 4.8088546\nepoch: 15 step: 11, loss is 11.506555\nepoch: 15 step: 12, loss is 7.6589003\nepoch: 15 step: 13, loss is 8.766976\nepoch: 15 step: 14, loss is 6.354591\nepoch: 15 step: 15, loss is 2.0595784\nepoch: 15 step: 16, loss is 13.30548\nepoch: 15 step: 17, loss is 13.182139\nepoch: 15 step: 18, loss is 14.078029\nepoch: 15 step: 19, loss is 7.398485\nepoch: 15 step: 20, loss is 9.63966\nepoch: 15 step: 21, loss is 9.2446165\nepoch: 15 step: 22, loss is 4.795825\nepoch: 15 step: 23, loss is 7.111811\nepoch: 15 step: 24, loss is 6.9724236\nepoch: 15 step: 25, loss is 5.2983108\nepoch: 15 step: 26, loss is 11.133734\nepoch: 15 step: 27, loss is 8.7746105\nepoch: 15 step: 28, loss is 11.002144\nepoch: 15 step: 29, loss is 11.520294\nepoch: 15 step: 30, loss is 10.2761545\nepoch: 15 step: 31, loss is 6.8567376\nepoch: 15 step: 32, loss is 12.027202\nepoch: 15 step: 33, loss is 9.365128\nepoch: 15 step: 34, loss is 6.727841\nepoch: 15 step: 35, loss is 7.9472227\nepoch: 15 step: 36, loss is 14.055729\nepoch: 15 step: 37, loss is 8.558161\nepoch: 15 step: 38, loss is 8.025396\nepoch: 15 step: 39, loss is 12.94637\nepoch: 15 step: 40, loss is 12.590244\nepoch: 15 step: 41, loss is 7.0679493\nepoch: 15 step: 42, loss is 7.8676567\nepoch: 15 step: 43, loss is 11.53104\nepoch: 15 step: 44, loss is 12.09459\nepoch: 15 step: 45, loss is 9.925833\nepoch: 15 step: 46, loss is 5.678198\nepoch: 15 step: 47, loss is 8.789598\nepoch: 15 step: 48, loss is 5.553914\nepoch: 15 step: 49, loss is 12.07597\nepoch: 15 step: 50, loss is 11.6872225\nepoch: 15 step: 51, loss is 7.049937\nepoch: 15 step: 52, loss is 6.8839245\nepoch: 15 step: 53, loss is 17.176062\nepoch: 15 step: 54, loss is 22.824257\nepoch: 15 step: 55, loss is 14.080478\nepoch: 15 step: 56, loss is 17.088715\nepoch: 15 step: 57, loss is 8.614108\nepoch: 15 step: 58, loss is 12.921793\nepoch: 15 step: 59, loss is 9.784421\nepoch: 15 step: 60, loss is 5.500412\nepoch: 15 step: 61, loss is 13.448537\nepoch: 15 step: 62, loss is 7.5361958\nepoch: 15 step: 63, loss is 12.326132\nepoch: 15 step: 64, loss is 12.61959\nepoch: 15 step: 65, loss is 10.095034\nepoch: 15 step: 66, loss is 12.3083515\nepoch: 15 step: 67, loss is 16.0758\nepoch: 15 step: 68, loss is 7.3987327\nepoch: 15 step: 69, loss is 10.662065\nepoch: 15 step: 70, loss is 11.628224\nepoch: 15 step: 71, loss is 11.630119\nepoch: 15 step: 72, loss is 8.207007\nepoch: 15 step: 73, loss is 19.821688\nepoch: 15 step: 74, loss is 15.661917\nepoch: 15 step: 75, loss is 6.628292\nepoch: 15 step: 76, loss is 14.820907\nepoch: 15 step: 77, loss is 8.162029\nepoch: 15 step: 78, loss is 7.384261\nepoch: 15 step: 79, loss is 8.110954\nepoch: 15 step: 80, loss is 8.722259\nepoch: 15 step: 81, loss is 5.4473004\nepoch: 15 step: 82, loss is 10.452867\nepoch: 15 step: 83, loss is 8.785101\nepoch: 15 step: 84, loss is 11.683755\nepoch: 15 step: 85, loss is 10.121477\nepoch: 15 step: 86, loss is 5.6362495\nepoch: 15 step: 87, loss is 16.79223\nepoch: 15 step: 88, loss is 6.813443\nepoch: 15 step: 89, loss is 11.770137\nepoch: 15 step: 90, loss is 11.071966\nepoch: 15 step: 91, loss is 7.0200725\nepoch: 15 step: 92, loss is 10.460428\nepoch: 15 step: 93, loss is 13.1711855\nepoch: 15 step: 94, loss is 8.011527\nepoch: 15 step: 95, loss is 9.204734\nepoch: 15 step: 96, loss is 12.905044\nepoch: 15 step: 97, loss is 6.3180637\nepoch: 15 step: 98, loss is 8.940808\nepoch: 15 step: 99, loss is 10.043462\nepoch: 15 step: 100, loss is 8.740906\nepoch: 15 step: 101, loss is 9.536756\nepoch: 15 step: 102, loss is 8.767975\nepoch: 15 step: 103, loss is 10.946829\nepoch: 15 step: 104, loss is 16.14537\nepoch: 15 step: 105, loss is 4.839939\nepoch: 15 step: 106, loss is 11.537703\nepoch: 15 step: 107, loss is 6.7793784\nepoch: 15 step: 108, loss is 8.90535\nepoch: 15 step: 109, loss is 13.465601\nepoch: 15 step: 110, loss is 5.2531495\nepoch: 15 step: 111, loss is 7.1628056\nepoch: 15 step: 112, loss is 10.86929\nepoch: 15 step: 113, loss is 9.124276\nepoch: 15 step: 114, loss is 9.827043\nepoch: 15 step: 115, loss is 9.361005\nepoch: 15 step: 116, loss is 7.4393253\nepoch: 15 step: 117, loss is 6.4685507\nepoch: 15 step: 118, loss is 2.9740012\nepoch: 15 step: 119, loss is 6.00922\nepoch: 15 step: 120, loss is 8.051991\nepoch: 15 step: 121, loss is 5.3493967\nepoch: 15 step: 122, loss is 4.5487843\nepoch: 15 step: 123, loss is 11.071266\nepoch: 15 step: 124, loss is 6.605011\nepoch: 15 step: 125, loss is 8.3628435\nepoch: 15 step: 126, loss is 11.274642\nepoch: 15 step: 127, loss is 10.597794\nepoch: 15 step: 128, loss is 6.168735\nepoch time: 43899.887 ms, per step time: 342.968 ms\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "evalList = []\nmask = [1] + [0 for i in range(MAX_SEQ_LEN - 1)]\n\nprovider = DataProvision(val_questions, val_answers, val_image_feature)\nquestion = []\ndecoder_in = []\nfeat = []\nlabel = []\nmax_loss = 0\nsum_loss = 0\nfor id, q in enumerate(val_questions):\n    question.append(wordEncode(q, word2id)[1:])\n    decoder_in.append(mask)\n    feat.append(val_image_feature[id])\n    label.append(wordEncode(val_answers[id], word2id)[1:])\n    if id % 32 < 31:\n        continue\n    question = Tensor(np.array(question, dtype=np.int32))\n    decoder_in = Tensor(np.array(decoder_in).astype(np.int32))\n    feat = Tensor(np.array(feat, dtype=np.float32))\n    label = Tensor(np.array(label, dtype=np.int32))\n    output = model.train_network(question, decoder_in, feat, label)\n    output = output.asnumpy()\n    max_loss = max(max_loss, output)\n    sum_loss += output\n#     evalList.append({\"answer\": wordDecode(output, id2word), \"question_id\": val_question_ids[id]})\n    question = []\n    decoder_in = []\n    feat = []\n    label = []\n    \nprint(\"Max Loss: \", max_loss)\nprint(\"Avg Loss: \", sum_loss * 32 / len(val_questions))\n\n# json.dump(evalList, open(cfg.resultFile, \"w\"))\n\n# vqa = VQA(cfg.valAnnFile, cfg.valQuesFile)\n# vqaEval = VQAEval(vqa, vqa.loadRes(cfg.resultFile, cfg.valQuesFile))\n# vqaEval.evaluate()\n# print(\"Overall Accuracy is: %.02f\\n\" %(vqaEval.accuracy['overall']))", "metadata": {"trusted": true}, "execution_count": 11, "outputs": [{"name": "stdout", "text": "Max Loss:  65.89157\nAvg Loss:  36.70064777135849\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}