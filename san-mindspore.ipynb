{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import re\n","import sys\n","import random\n","import unicodedata\n","import math\n","\n","from mindspore import Tensor, nn, Model, context\n","from mindspore.train.serialization import load_param_into_net, load_checkpoint\n","from mindspore.train.callback import LossMonitor, CheckpointConfig, ModelCheckpoint, TimeMonitor\n","from mindspore import dataset as ds\n","from mindspore.mindrecord import FileWriter\n","from mindspore import Parameter\n","from mindspore.nn.loss.loss import _Loss\n","from mindspore.ops import functional as F\n","from mindspore.ops import operations as P\n","from mindspore.common import dtype as mstype"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from easydict import EasyDict as edict\n","\n","# CONFIG\n","cfg = edict({\n","    'en_vocab_size': 1154,\n","    'ch_vocab_size': 1116,\n","    'max_seq_length': 10,\n","    'hidden_size': 1024,\n","    'batch_size': 16,\n","    'eval_batch_size': 1,\n","    'learning_rate': 0.001,\n","    'momentum': 0.9,\n","    'num_epochs': 15,\n","    'save_checkpoint_steps': 125,\n","    'keep_checkpoint_max': 10,\n","    'dataset_path':'./preprocess',\n","    'ckpt_save_path':'./ckpt',\n","    'checkpoint_path':'./ckpt/gru-15_125.ckpt'\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Attention(nn.cell):\n","    def __init__(self, config, is_training=True):\n","        super(Attention, self).__init__()\n","        self.hidden_size = config.hidden_size\n","        self.attnq = nn.Dense(self.hidden_size, self.hidden_size)\n","        self.attni = nn.Dense(self.hidden_size, self.hidden_size)\n","        self.attnp = nn.Dense(self.hidden_size, 1, activation = \"softmax\")\n","        self.add = P.Add()\n","        self.mul = P.Mul()\n","\n","    def sum(x):\n","        x = np.array(x)\n","        y = np.zeros((x.shape[0], x.shape[2])).astype(x.dtype)\n","        for i in range(0,x.shape[0]):\n","            for j in range(0,x.shape[1]):\n","                for k in range(0,x.shape[2]):\n","                    y[i][k] += x[i][j][k]\n","        return Tensor(y)\n","\n","    def construct(self, question, img):\n","        i_attn = self.attni(img)\n","        q_attn = self.attnq(question)\n","        ha = nn.tanh(self.add(i_attn, q_attn))\n","        p = self.attnp(ha)\n","        u = self.add(self.sum(self.mul(p,img)), question)\n","        return u\n","\n","class SAN(nn.cell):\n","    def __init__(self, config, is_training=True):\n","        super(SAN, self).__init__()\n","        self.hidden_size = config.hidden_size\n","        self.attn_1 = Attention(config = config, is_training = is_training)\n","        self.attn_2 = Attention(config = config, is_training = is_training)\n","        self.attn_3 = Attention(config = config, is_training = is_training)\n","\n","    def construct(self, question, img):\n","\n","        u_1 = self.attn_1(question,img)\n","        u_2 = self.attn_2(u_1,img)\n","        u_3 = self.attn_3(u_2,img)\n","\n","        return u_3\n","        \n","class Encoder(nn.Cell):\n","    def __init__(self, config, is_training=True):\n","        super(Encoder, self).__init__()\n","        self.vocab_size = config.en_vocab_size\n","        self.hidden_size = config.hidden_size\n","        if is_training:\n","            self.batch_size = config.batch_size\n","        else:\n","            self.batch_size = config.eval_batch_size\n","\n","        self.trans = P.Transpose()\n","        self.perm = (1, 0, 2)\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n","        self.cnn = CNN(config, is_training=is_training)\n","        self.h = Tensor(np.zeros((self.batch_size, self.hidden_size)).astype(np.float16))\n","\n","    def construct(self, encoder_input, img_input):\n","        embeddings = self.embedding(encoder_input)\n","        embeddings = self.trans(embeddings, self.perm)\n","        output, hidden = self.gru(embeddings, self.h)\n","        img_output = self.cnn(img_input)\n","        return output, hidden, img_output\n","\n","class Decoder(nn.Cell):\n","    def __init__(self, config, is_training=True, dropout=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.vocab_size = config.ch_vocab_size\n","        self.hidden_size = config.hidden_size\n","        self.max_len = config.max_seq_length\n","\n","        self.trans = P.Transpose()\n","        self.perm = (1, 0, 2)\n","        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n","        self.dropout = nn.Dropout(1-dropout)\n","        self.attn = nn.Dense(self.hidden_size, self.max_len)\n","        self.softmax = nn.Softmax(axis=2)\n","        self.bmm = P.BatchMatMul()\n","        self.concat = P.Concat(axis=2)\n","        self.attn_combine = nn.Dense(self.hidden_size * 2, self.hidden_size)\n","\n","        self.gru = GRU(config, is_training=is_training).to_float(mstype.float16)\n","        self.out = nn.Dense(self.hidden_size, self.vocab_size)\n","        self.logsoftmax = nn.LogSoftmax(axis=2)\n","        self.cast = P.Cast()\n","\n","    def construct(self, decoder_input, hidden, encoder_output):\n","        embeddings = self.embedding(decoder_input)\n","        embeddings = self.dropout(embeddings)\n","        # calculate attn\n","        attn_weights = self.softmax(self.attn(embeddings))\n","        encoder_output = self.trans(encoder_output, self.perm)\n","        attn_applied = self.bmm(attn_weights, self.cast(encoder_output,mstype.float32))\n","        output = self.concat((embeddings, attn_applied))\n","        output = self.attn_combine(output)\n","\n","\n","        embeddings = self.trans(embeddings, self.perm)\n","        output, hidden = self.gru(embeddings, hidden)\n","        output = self.cast(output, mstype.float32)\n","        output = self.out(output)\n","        output = self.logsoftmax(output)\n","\n","        return output, hidden, attn_weights\n","\n","class Seq2Seq(nn.Cell):\n","    def __init__(self, config, is_train=True):\n","        super(Seq2Seq, self).__init__()\n","        self.max_len = config.max_seq_length\n","        self.is_train = is_train\n","\n","        self.encoder = Encoder(config, is_train)\n","        self.decoder = Decoder(config, is_train)\n","        self.expanddims = P.ExpandDims()\n","        self.squeeze = P.Squeeze(axis=0)\n","        self.argmax = P.ArgMaxWithValue(axis=int(2), keep_dims=True)\n","        self.concat = P.Concat(axis=1)\n","        self.concat2 = P.Concat(axis=0)\n","        self.select = P.Select()\n","        self.san = SAN(config,is_train)\n","\n","    def construct(self, src, dst, img):\n","        encoder_output, hidden = self.encoder(src)\n","        img_output = self.cnn(img)\n","        san_out = self.san(encoder_output,img_output)\n","        \n","        decoder_hidden = self.squeeze(encoder_output[self.max_len-2:self.max_len-1:1, ::, ::])\n","        if self.is_train:\n","            outputs, _ = self.decoder(dst, decoder_hidden, san_out)\n","        else:\n","            decoder_input = dst[::,0:1:1]\n","            decoder_outputs = ()\n","            for i in range(0, self.max_len):\n","                decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n","                                                                 decoder_hidden, san_out)\n","                decoder_hidden = self.squeeze(decoder_hidden)\n","                decoder_output, _ = self.argmax(decoder_output)\n","                decoder_output = self.squeeze(decoder_output)\n","                decoder_outputs += (decoder_output,)\n","                decoder_input = decoder_output\n","            outputs = self.concat(decoder_outputs)\n","        # if self.is_train:\n","        #     outputs, _ = self.decoder(dst, decoder_hidden, encoder_output)\n","        # else:\n","        #     decoder_input = dst[::,0:1:1]\n","        #     decoder_outputs = ()\n","        #     for i in range(0, self.max_len):\n","        #         decoder_output, decoder_hidden, _ = self.decoder(decoder_input, \n","        #                                                          decoder_hidden, encoder_output)\n","        #         decoder_hidden = self.squeeze(decoder_hidden)\n","        #         decoder_output, _ = self.argmax(decoder_output)\n","        #         decoder_output = self.squeeze(decoder_output)\n","        #         decoder_outputs += (decoder_output,)\n","        #         decoder_input = decoder_output\n","        #     outputs = self.concat(decoder_outputs)\n","        return outputs\n","\n","class NLLLoss(_Loss):\n","    '''\n","       NLLLoss function\n","    '''\n","    def __init__(self, reduction='mean'):\n","        super(NLLLoss, self).__init__(reduction)\n","        self.one_hot = P.OneHot()\n","        self.reduce_sum = P.ReduceSum()\n","\n","    def construct(self, logits, label):\n","        label_one_hot = self.one_hot(label, F.shape(logits)[-1], F.scalar_to_array(1.0), \n","                                     F.scalar_to_array(0.0))\n","        #print('NLLLoss label_one_hot:',label_one_hot, label_one_hot.shape)\n","        #print('NLLLoss logits:',logits, logits.shape)\n","        #print('xxx:', logits * label_one_hot)\n","        loss = self.reduce_sum(-1.0 * logits * label_one_hot, (1,))\n","        return self.get_loss(loss)\n","    \n","class WithLossCell(nn.Cell):\n","    def __init__(self, backbone, config):\n","        super(WithLossCell, self).__init__(auto_prefix=False)\n","        self._backbone = backbone\n","        self.batch_size = config.batch_size\n","        self.onehot = nn.OneHot(depth=config.ch_vocab_size)\n","        self._loss_fn = NLLLoss()\n","        self.max_len = config.max_seq_length\n","        self.squeeze = P.Squeeze()\n","        self.cast = P.Cast()\n","        self.argmax = P.ArgMaxWithValue(axis=1, keep_dims=True)\n","        self.print = P.Print()\n","\n","    def construct(self, src, dst, label):\n","        out = self._backbone(src, dst)\n","        loss_total = 0\n","        for i in range(self.batch_size):\n","            loss = self._loss_fn(self.squeeze(out[::,i:i+1:1,::]), \n","                                 self.squeeze(label[i:i+1:1, ::]))\n","            loss_total += loss\n","        loss = loss_total / self.batch_size\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["network = Seq2Seq(cfg)\n","network = WithLossCell(network, cfg)\n","optimizer = nn.Adam(network.trainable_params(), learning_rate=cfg.learning_rate, beta1=0.9, beta2=0.98)\n","model = Model(network, optimizer=optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["loss_cb = LossMonitor()\n","config_ck = CheckpointConfig(save_checkpoint_steps=cfg.save_checkpoint_steps, keep_checkpoint_max=cfg.keep_checkpoint_max)\n","ckpoint_cb = ModelCheckpoint(prefix=\"gru\", directory=cfg.ckpt_save_path, config=config_ck)\n","time_cb = TimeMonitor(data_size=ds_train.get_dataset_size())\n","callbacks = [time_cb, ckpoint_cb, loss_cb]\n","\n","model.train(cfg.num_epochs, ds_train, callbacks=callbacks, dataset_sink_mode=True)"]}],"metadata":{"kernelspec":{"display_name":"MindSpore-python3.7-aarch64","language":"python","name":"mindspore-python3.7-aarch64"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":4}
